{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction Modern services are comprised of many dynamic variables, that need to be changed regularly. Today, the process is unstructured and error prone. From ML model variables, kill switches, gradual rollout configuration, A/B experiment configuration and more - developers want their code to allow to be configured to the finer details. Protoconf is a modern approach to software configuration , inspired by Facebook's Configerator . Using Protoconf enables: Code review for configuration changes Enables the battle tested flow of pull-request & code-review. Configuration auditing out of the box (who did what, when?). The repository is the source of truth for the configuration deployed to production. No service restart required to pick up changes Instant delivery of configuration updates. Encourages writing software that doesn't know downtime. Clear representation of complex configuration Write configuration in Starlark (a Python dialect), no more copying & pasting from huge JSON files. Automated validation Config follows a fully-typed (Protobuf) schema. This allows writing validation code in Starlark, to verify your configuration before it is committed. What is protoconf Protoconf is a configuration management framework. We call it a framework because it provides a platform to manage the entire life cycle of configuration files (configs). Protoconf is a tool to aid in the specification and distribution of configs. The goals of specification are to be robust, composable, and less error-prone. The goals of distribution are to reach all of your machines quickly, reliably, and to be highly available even in disaster scenarios. When should you use Protoconf? You need to update and distribute configuration dynamically, often to a large number of hosts or services. You want to write your configuration with code. You need change history. You want to code review config changes. You want validation that config changes conform to a schema and do not violate invariants that you define. You want to canary (test) config changes before distributing them to production. You can tolerate eventual consistency; config updates are not atomic w.r.t. different consumers. You don't need config updates to propagate to your consumers within a certain SLA. Your configs are reasonably small You don't need very frequent config updates, more than about once every 5 mins. How Protoconf works Configuration update flow How this looks from the service's eyes This is roughly how configuration is consumed by a service. This paradigm encourages you to write software that can reconfigure itself in runtime rather than require a restart: Python #!/usr/bin/env python import grpc from v1.protoconf_service_pb2_grpc import ProtoconfServiceStub from v1.protoconf_service_pb2 import ConfigSubscriptionRequest from myproject.myconfig_pb2 import MyConfig channel = grpc . insecure_channel ( \"localhost:4300\" ) stub = ProtoconfServiceStub ( channel ) config = MyConfig () for update in stub . SubscribeForConfig ( ConfigSubscriptionRequest ( path = \"myproject/myconfig\" )): # Override `config` update . value . Unpack ( config ) print ( config ) As Protoconf uses Protobuf and gRPC, it supports delivering configuration to all major languages . See also: Protobuf overview .","title":"Home"},{"location":"#introduction","text":"Modern services are comprised of many dynamic variables, that need to be changed regularly. Today, the process is unstructured and error prone. From ML model variables, kill switches, gradual rollout configuration, A/B experiment configuration and more - developers want their code to allow to be configured to the finer details. Protoconf is a modern approach to software configuration , inspired by Facebook's Configerator . Using Protoconf enables: Code review for configuration changes Enables the battle tested flow of pull-request & code-review. Configuration auditing out of the box (who did what, when?). The repository is the source of truth for the configuration deployed to production. No service restart required to pick up changes Instant delivery of configuration updates. Encourages writing software that doesn't know downtime. Clear representation of complex configuration Write configuration in Starlark (a Python dialect), no more copying & pasting from huge JSON files. Automated validation Config follows a fully-typed (Protobuf) schema. This allows writing validation code in Starlark, to verify your configuration before it is committed.","title":"Introduction"},{"location":"#what-is-protoconf","text":"Protoconf is a configuration management framework. We call it a framework because it provides a platform to manage the entire life cycle of configuration files (configs). Protoconf is a tool to aid in the specification and distribution of configs. The goals of specification are to be robust, composable, and less error-prone. The goals of distribution are to reach all of your machines quickly, reliably, and to be highly available even in disaster scenarios.","title":"What is protoconf"},{"location":"#when-should-you-use-protoconf","text":"You need to update and distribute configuration dynamically, often to a large number of hosts or services. You want to write your configuration with code. You need change history. You want to code review config changes. You want validation that config changes conform to a schema and do not violate invariants that you define. You want to canary (test) config changes before distributing them to production. You can tolerate eventual consistency; config updates are not atomic w.r.t. different consumers. You don't need config updates to propagate to your consumers within a certain SLA. Your configs are reasonably small You don't need very frequent config updates, more than about once every 5 mins.","title":"When should you use Protoconf?"},{"location":"#how-protoconf-works","text":"","title":"How Protoconf works"},{"location":"#configuration-update-flow","text":"","title":"Configuration update flow"},{"location":"#how-this-looks-from-the-services-eyes","text":"This is roughly how configuration is consumed by a service. This paradigm encourages you to write software that can reconfigure itself in runtime rather than require a restart: Python #!/usr/bin/env python import grpc from v1.protoconf_service_pb2_grpc import ProtoconfServiceStub from v1.protoconf_service_pb2 import ConfigSubscriptionRequest from myproject.myconfig_pb2 import MyConfig channel = grpc . insecure_channel ( \"localhost:4300\" ) stub = ProtoconfServiceStub ( channel ) config = MyConfig () for update in stub . SubscribeForConfig ( ConfigSubscriptionRequest ( path = \"myproject/myconfig\" )): # Override `config` update . value . Unpack ( config ) print ( config ) As Protoconf uses Protobuf and gRPC, it supports delivering configuration to all major languages . See also: Protobuf overview .","title":"How this looks from the service's eyes"},{"location":"getting-started/","text":"Getting Started Prerequisite for this guide: Knowledge in Python Familiarity with Protobuf and gRPC Define your first config The first step will be to define the config struct in protobuf . The protobuf file will be used to generate a marshaler of in the language of choice which can be used alongside the gRPC client to pull configs from the protoconf agent gRPC endpoint. // file: ./src/myproject/myconfig.proto syntax = \"proto3\" ; message MyConfig { uint32 connection_timeout = 1 ; uint32 max_retries = 2 ; NestedStruct another_struct = 3 ; } message NestedStruct { string hello_world = 1 ; } Code your config Create a .pconf file to populate the config struct with the required values. \"\"\" file: ./src/myproject/myconfig.pconf \"\"\" load ( \"myconfig.proto\" , \"MyConfig\" , \"NestedStruct\" ) def main (): return MyConfig ( connection_timeout = 5 , max_retries = 5 , another_struct = NestedStruct ( hello_world = \"Hello World!\" ) ) Compile and check results Our working directory is now ready to be compiled. Run protoconf compile . . The compiler will create a new file under materialized_configs/myproject/myconfig.materialized_JSON which can be used to validate the result of the config. // f ile : ma ter ialized_co nf igs/myprojec t /myco nf ig.ma ter ialized_JSON { \"protoFile\" : \"myproject/myconfig.proto\" , \"value\" : { \"@type\" : \"type.googleapis.com/MyConfig\" , \"connectionTimeout\" : 5 , \"maxRetries\" : 5 , \"anotherStruct\" : { \"helloWorld\" : \"Hello World!\" } } } Add validators You might want to make sure no one can accidentally reduce the connection_timeout config below 3 . If he wish to do so, he can add a validator to the MyConfig struct: \"\"\" file: ./src/myproject/myconfig.proto-validator \"\"\" load ( \"myconfig.proto\" , \"MyConfig\" ) def validate_connection_timeout ( config ): if config . connection_timeout <= 3 : fail ( \"connection_timeout must be 3 or higher, got: %d \" % config . connection_timeout ) add_validator ( MyConfig , validate_connection_timeout ) Consume your config locally To test his configs locally, you can run protoconf agent -dev . The agent is now running and listening on 0.0.0.0:4300 and ready to accept gRPC calls. Install the grpc and protobuf tools to generate the stub code to communicate with the protoconf gRPC agent. $ pip install grpcio-tools $ python -m grpc_tools.protoc -Isrc --python_out = . --grpc_python_out = . ./src/myproject/myconfig.proto $ git clone https://github.com/protoconf/protoconf.git $ python -m grpc_tools.protoc -Iprotoconf/agent/api/proto/ --python_out = . --grpc_python_out = . protoconf/agent/api/proto/v1/protoconf_service.proto Write a simple python code that will use the generated code to communicate with the agent. #!/usr/bin/env python import grpc from v1.protoconf_service_pb2_grpc import ProtoconfServiceStub from v1.protoconf_service_pb2 import ConfigSubscriptionRequest from myproject.myconfig_pb2 import MyConfig channel = grpc . insecure_channel ( \"localhost:4300\" ) stub = ProtoconfServiceStub ( channel ) config = MyConfig () for update in stub . SubscribeForConfig ( ConfigSubscriptionRequest ( path = \"myproject/myconfig\" )): update . value . Unpack ( config ) print ( config ) Run the python code and make a change to the ./src/myproject/myconfig.pconf . After running protoconf compile . again, you will see the config changes in your running software. Prepare for Production Use a supported KV store to release the config to production. The supported storages are: Consul , Etcd or Zookeeper . $ consul agent -dev & $ protoconf insert -store consul -store-address localhost:8500 . myproject/myconfig Run the agent in production mode $ protoconf agent -store consul -store-address localhost:8500 Run your code the same way as step 5. Then make a change, compile and run the protoconf insert command from step 6 again.","title":"Getting Started"},{"location":"getting-started/#getting-started","text":"Prerequisite for this guide: Knowledge in Python Familiarity with Protobuf and gRPC","title":"Getting Started"},{"location":"getting-started/#define-your-first-config","text":"The first step will be to define the config struct in protobuf . The protobuf file will be used to generate a marshaler of in the language of choice which can be used alongside the gRPC client to pull configs from the protoconf agent gRPC endpoint. // file: ./src/myproject/myconfig.proto syntax = \"proto3\" ; message MyConfig { uint32 connection_timeout = 1 ; uint32 max_retries = 2 ; NestedStruct another_struct = 3 ; } message NestedStruct { string hello_world = 1 ; }","title":"Define your first config"},{"location":"getting-started/#code-your-config","text":"Create a .pconf file to populate the config struct with the required values. \"\"\" file: ./src/myproject/myconfig.pconf \"\"\" load ( \"myconfig.proto\" , \"MyConfig\" , \"NestedStruct\" ) def main (): return MyConfig ( connection_timeout = 5 , max_retries = 5 , another_struct = NestedStruct ( hello_world = \"Hello World!\" ) )","title":"Code your config"},{"location":"getting-started/#compile-and-check-results","text":"Our working directory is now ready to be compiled. Run protoconf compile . . The compiler will create a new file under materialized_configs/myproject/myconfig.materialized_JSON which can be used to validate the result of the config. // f ile : ma ter ialized_co nf igs/myprojec t /myco nf ig.ma ter ialized_JSON { \"protoFile\" : \"myproject/myconfig.proto\" , \"value\" : { \"@type\" : \"type.googleapis.com/MyConfig\" , \"connectionTimeout\" : 5 , \"maxRetries\" : 5 , \"anotherStruct\" : { \"helloWorld\" : \"Hello World!\" } } }","title":"Compile and check results"},{"location":"getting-started/#add-validators","text":"You might want to make sure no one can accidentally reduce the connection_timeout config below 3 . If he wish to do so, he can add a validator to the MyConfig struct: \"\"\" file: ./src/myproject/myconfig.proto-validator \"\"\" load ( \"myconfig.proto\" , \"MyConfig\" ) def validate_connection_timeout ( config ): if config . connection_timeout <= 3 : fail ( \"connection_timeout must be 3 or higher, got: %d \" % config . connection_timeout ) add_validator ( MyConfig , validate_connection_timeout )","title":"Add validators"},{"location":"getting-started/#consume-your-config-locally","text":"To test his configs locally, you can run protoconf agent -dev . The agent is now running and listening on 0.0.0.0:4300 and ready to accept gRPC calls. Install the grpc and protobuf tools to generate the stub code to communicate with the protoconf gRPC agent. $ pip install grpcio-tools $ python -m grpc_tools.protoc -Isrc --python_out = . --grpc_python_out = . ./src/myproject/myconfig.proto $ git clone https://github.com/protoconf/protoconf.git $ python -m grpc_tools.protoc -Iprotoconf/agent/api/proto/ --python_out = . --grpc_python_out = . protoconf/agent/api/proto/v1/protoconf_service.proto Write a simple python code that will use the generated code to communicate with the agent. #!/usr/bin/env python import grpc from v1.protoconf_service_pb2_grpc import ProtoconfServiceStub from v1.protoconf_service_pb2 import ConfigSubscriptionRequest from myproject.myconfig_pb2 import MyConfig channel = grpc . insecure_channel ( \"localhost:4300\" ) stub = ProtoconfServiceStub ( channel ) config = MyConfig () for update in stub . SubscribeForConfig ( ConfigSubscriptionRequest ( path = \"myproject/myconfig\" )): update . value . Unpack ( config ) print ( config ) Run the python code and make a change to the ./src/myproject/myconfig.pconf . After running protoconf compile . again, you will see the config changes in your running software.","title":"Consume your config locally"},{"location":"getting-started/#prepare-for-production","text":"Use a supported KV store to release the config to production. The supported storages are: Consul , Etcd or Zookeeper . $ consul agent -dev & $ protoconf insert -store consul -store-address localhost:8500 . myproject/myconfig","title":"Prepare for Production"},{"location":"getting-started/#run-the-agent-in-production-mode","text":"$ protoconf agent -store consul -store-address localhost:8500 Run your code the same way as step 5. Then make a change, compile and run the protoconf insert command from step 6 again.","title":"Run the agent in production mode"},{"location":"installation/","text":"Installation On Linux/MacOS export PROTOCONF_VERSION=\"0.1.5\" export PROTOCONF_OS=$(uname | tr '[A-Z]' '[a-z]') # change to \"arm64\" if needed export PROTOCONF_ARCH=\"amd64\" curl -LO https://github.com/protoconf/protoconf/releases/download/${PROTOCONF_VERSION}/protoconf-${PROTOCONF_OS}-${PROTOCONF_ARCH}-${PROTOCONF_VERSION}.tar.gz sudo tar xvf protoconf-${PROTOCONF_OS}-${PROTOCONF_ARCH}-${PROTOCONF_VERSION}.tar.gz -C /usr/local/bin On Windows Download from the github releases page . Validate the installation $ protoconf 2020/04/26 10:16:59 proto: duplicate proto type registered: v1.ConfigSubscriptionRequest 2020/04/26 10:16:59 proto: duplicate proto type registered: v1.ConfigUpdate Usage: protoconf [--version] [--help] <command> [<args>] Available commands are: agent Runs a Protoconf agent compile Compile configs exec Watches keys and execute on changes import insert Insert a materialized config to the key-value store mutate Write to mutation server serve Runs a server","title":"Installation"},{"location":"installation/#installation","text":"","title":"Installation"},{"location":"installation/#on-linuxmacos","text":"export PROTOCONF_VERSION=\"0.1.5\" export PROTOCONF_OS=$(uname | tr '[A-Z]' '[a-z]') # change to \"arm64\" if needed export PROTOCONF_ARCH=\"amd64\" curl -LO https://github.com/protoconf/protoconf/releases/download/${PROTOCONF_VERSION}/protoconf-${PROTOCONF_OS}-${PROTOCONF_ARCH}-${PROTOCONF_VERSION}.tar.gz sudo tar xvf protoconf-${PROTOCONF_OS}-${PROTOCONF_ARCH}-${PROTOCONF_VERSION}.tar.gz -C /usr/local/bin","title":"On Linux/MacOS"},{"location":"installation/#on-windows","text":"Download from the github releases page .","title":"On Windows"},{"location":"installation/#validate-the-installation","text":"$ protoconf 2020/04/26 10:16:59 proto: duplicate proto type registered: v1.ConfigSubscriptionRequest 2020/04/26 10:16:59 proto: duplicate proto type registered: v1.ConfigUpdate Usage: protoconf [--version] [--help] <command> [<args>] Available commands are: agent Runs a Protoconf agent compile Compile configs exec Watches keys and execute on changes import insert Insert a materialized config to the key-value store mutate Write to mutation server serve Runs a server","title":"Validate the installation"},{"location":"multiple-outputs/","text":"Multiple outputs Sometimes, we want to generate multiple configs from a single file. for this, we can use the .mpconf . protoconf expects .mpconf 's main() function to return a dict with a string as the key and a proto.Message as the value. Example: // file: src/myservice/myconfig.proto syntax = \"proto3\" ; message MyConfig { string name = 1 ; uint32 timeout = 2 ; } \"\"\" file: ./src/myservice/outputs.mpconf \"\"\" load ( \"myconfig.proto\" , \"MyConfig\" ) out = {} for i in range ( 5 ): out [ \"config %d \" % i ] = MyConfig ( name = \"config %d \" % i , timeout = i * 3 ) def main (): return out Now, when you compile your configs, you will see multiple outputs $ protoconf compile . $ find materialized_config/myservice materialized_config/myservice materialized_config/myservice/outputs materialized_config/myservice/outputs/config4.materialized_JSON materialized_config/myservice/outputs/config1.materialized_JSON materialized_config/myservice/outputs/config0.materialized_JSON materialized_config/myservice/outputs/config3.materialized_JSON materialized_config/myservice/outputs/config2.materialized_JSON","title":"Multiple Outputs"},{"location":"multiple-outputs/#multiple-outputs","text":"Sometimes, we want to generate multiple configs from a single file. for this, we can use the .mpconf . protoconf expects .mpconf 's main() function to return a dict with a string as the key and a proto.Message as the value. Example: // file: src/myservice/myconfig.proto syntax = \"proto3\" ; message MyConfig { string name = 1 ; uint32 timeout = 2 ; } \"\"\" file: ./src/myservice/outputs.mpconf \"\"\" load ( \"myconfig.proto\" , \"MyConfig\" ) out = {} for i in range ( 5 ): out [ \"config %d \" % i ] = MyConfig ( name = \"config %d \" % i , timeout = i * 3 ) def main (): return out Now, when you compile your configs, you will see multiple outputs $ protoconf compile . $ find materialized_config/myservice materialized_config/myservice materialized_config/myservice/outputs materialized_config/myservice/outputs/config4.materialized_JSON materialized_config/myservice/outputs/config1.materialized_JSON materialized_config/myservice/outputs/config0.materialized_JSON materialized_config/myservice/outputs/config3.materialized_JSON materialized_config/myservice/outputs/config2.materialized_JSON","title":"Multiple outputs"},{"location":"mutation-rpc/","text":"Mutation RPC usage One of the core principals of protoconf is the ability to mutate configs via an API (or RPC). This allows humans and machines work together on the same configuration codebase. protoconf allow humans to code the logic, while machines can only change values via the RPC. Create a dummy config // file: ./src/myservice/myconfig.proto syntax = \"proto3\" ; message MyConfig { string name = 1 ; uint32 timeout = 2 ; } \"\"\" file: ./src/myservice/default.pconf \"\"\" load ( \"myconfig.proto\" , \"MyConfig\" ) config = MyConfig ( name = \"config\" ) def main (): return config $ protoconf compile . Create a post-mutation script $ echo '#!/bin/bash\\nprotoconf compile .' > post.sh $ chmod +x post.sh Run the mutation server in the background $ protoconf serve -post ./post.sh . & Now we will use the protoconf mutate command to hit the mutation RPC $ protoconf mutate -path myservice/mutation -proto myservice/myconfig.proto -msg MyConfig -field timeout = 3 You will now notice a new file created under mutable_config $ find mutable_config mutable_config mutable_config/myservice mutable_config/myservice/mutation.materialized_JSON load the mutated values \"\"\" file: ./src/myservice/default.pconf \"\"\" load ( \"myconfig.proto\" , \"MyConfig\" ) load ( \"mutable:myservice/mutation\" , \"value\" ) config = MyConfig ( name = \"config\" , timeout = value . timeout ) def main (): return config Run the protoconf mutate command again with different value and watch how your configs changes. Next steps Running in production The protoconf serve command accepts -pre and -post scripts which should be used for preparing the ground for writing the mutation ( -pre ) and followup actions to run after writing the mutation ( -post ). Both scripts will run by protoconf serve on every mutation. The scripts will be receiveing a metadata string as its first argument ( $1 in bash ) and can be used to pass metadata from the initiator of the rpc to the script, This can be used to pass a token for github to validate the initiator or to pass more context to be added to the commit message. These scripts should handle the git lifecycle of the mutation (setting the workspace to latest ref, and push the result after the writing done.) compiling the configs should be part of the -post script. When running in HA, you can use these scripts to acquire a lock from consul / etcd . Using gRPC The mutation proto is available here .","title":"Mutation RPC"},{"location":"mutation-rpc/#mutation-rpc-usage","text":"One of the core principals of protoconf is the ability to mutate configs via an API (or RPC). This allows humans and machines work together on the same configuration codebase. protoconf allow humans to code the logic, while machines can only change values via the RPC.","title":"Mutation RPC usage"},{"location":"mutation-rpc/#create-a-dummy-config","text":"// file: ./src/myservice/myconfig.proto syntax = \"proto3\" ; message MyConfig { string name = 1 ; uint32 timeout = 2 ; } \"\"\" file: ./src/myservice/default.pconf \"\"\" load ( \"myconfig.proto\" , \"MyConfig\" ) config = MyConfig ( name = \"config\" ) def main (): return config $ protoconf compile .","title":"Create a dummy config"},{"location":"mutation-rpc/#create-a-post-mutation-script","text":"$ echo '#!/bin/bash\\nprotoconf compile .' > post.sh $ chmod +x post.sh","title":"Create a post-mutation script"},{"location":"mutation-rpc/#run-the-mutation-server-in-the-background","text":"$ protoconf serve -post ./post.sh . & Now we will use the protoconf mutate command to hit the mutation RPC $ protoconf mutate -path myservice/mutation -proto myservice/myconfig.proto -msg MyConfig -field timeout = 3 You will now notice a new file created under mutable_config $ find mutable_config mutable_config mutable_config/myservice mutable_config/myservice/mutation.materialized_JSON","title":"Run the mutation server in the background"},{"location":"mutation-rpc/#load-the-mutated-values","text":"\"\"\" file: ./src/myservice/default.pconf \"\"\" load ( \"myconfig.proto\" , \"MyConfig\" ) load ( \"mutable:myservice/mutation\" , \"value\" ) config = MyConfig ( name = \"config\" , timeout = value . timeout ) def main (): return config Run the protoconf mutate command again with different value and watch how your configs changes.","title":"load the mutated values"},{"location":"mutation-rpc/#next-steps","text":"","title":"Next steps"},{"location":"mutation-rpc/#running-in-production","text":"The protoconf serve command accepts -pre and -post scripts which should be used for preparing the ground for writing the mutation ( -pre ) and followup actions to run after writing the mutation ( -post ). Both scripts will run by protoconf serve on every mutation. The scripts will be receiveing a metadata string as its first argument ( $1 in bash ) and can be used to pass metadata from the initiator of the rpc to the script, This can be used to pass a token for github to validate the initiator or to pass more context to be added to the commit message. These scripts should handle the git lifecycle of the mutation (setting the workspace to latest ref, and push the result after the writing done.) compiling the configs should be part of the -post script. When running in HA, you can use these scripts to acquire a lock from consul / etcd .","title":"Running in production"},{"location":"mutation-rpc/#using-grpc","text":"The mutation proto is available here .","title":"Using gRPC"},{"location":"protoconf-exec/","text":"Using protoconf exec It's very likely that your infra relies on many components which does not have native protoconf integration. You can still use protoconf to code their config and use some wrappers around the process you are running in order to write and reload the config. protoconf exec aims to be a general purpose way to do so (still WIP and many features are not implemented) Import the exec config to your workspace $ mkdir -p src/exec curl -Lo src/exec/exec_config.proto https://raw.githubusercontent.com/protoconf/protoconf/v0.1.3/exec/config/exec_config.proto Create a dummy proto and config // file: ./src/myservice/myconfig.proto syntax = \"proto3\" ; message MyConfig { string name = 1 ; uint32 timeout = 2 ; } \"\"\" file: ./src/myservice/myconfig.pconf \"\"\" load ( \"myconfig.proto\" , \"MyConfig\" ) config = MyConfig ( name = \"test\" , timeout = 3 ) def main (): return config Create an exec config \"\"\" Generates tf.json files under ./tfconfigs \"\"\" load ( \"//exec/exec_config.proto\" , \"Config\" , \"WatcherConfig\" , \"Action\" , \"ActionTypeWriteToFile\" , ) configs = [ \"mysservice/myconfig\" ] def main (): return Config ( items = [ WatcherConfig ( path = path , proto_file = \"myservice/myconfig.proto\" , actions = [ Action ( file = ActionTypeWriteToFile ( path = \"tfconfigs/ %s .json\" % path )) ], ) for path in configs ] )","title":"Protoconf Exec"},{"location":"protoconf-exec/#using-protoconf-exec","text":"It's very likely that your infra relies on many components which does not have native protoconf integration. You can still use protoconf to code their config and use some wrappers around the process you are running in order to write and reload the config. protoconf exec aims to be a general purpose way to do so (still WIP and many features are not implemented)","title":"Using protoconf exec"},{"location":"protoconf-exec/#import-the-exec-config-to-your-workspace","text":"$ mkdir -p src/exec curl -Lo src/exec/exec_config.proto https://raw.githubusercontent.com/protoconf/protoconf/v0.1.3/exec/config/exec_config.proto","title":"Import the exec config to your workspace"},{"location":"protoconf-exec/#create-a-dummy-proto-and-config","text":"// file: ./src/myservice/myconfig.proto syntax = \"proto3\" ; message MyConfig { string name = 1 ; uint32 timeout = 2 ; } \"\"\" file: ./src/myservice/myconfig.pconf \"\"\" load ( \"myconfig.proto\" , \"MyConfig\" ) config = MyConfig ( name = \"test\" , timeout = 3 ) def main (): return config","title":"Create a dummy proto and config"},{"location":"protoconf-exec/#create-an-exec-config","text":"\"\"\" Generates tf.json files under ./tfconfigs \"\"\" load ( \"//exec/exec_config.proto\" , \"Config\" , \"WatcherConfig\" , \"Action\" , \"ActionTypeWriteToFile\" , ) configs = [ \"mysservice/myconfig\" ] def main (): return Config ( items = [ WatcherConfig ( path = path , proto_file = \"myservice/myconfig.proto\" , actions = [ Action ( file = ActionTypeWriteToFile ( path = \"tfconfigs/ %s .json\" % path )) ], ) for path in configs ] )","title":"Create an exec config"},{"location":"structuring-your-code/","text":"Structuring your code Sometimes, when we use protoconf we will want to write helpers functions and global constants that we might want to include in multiple configs. We can define those in .pinc files. .pinc files are starlark code which doesn't produce configs (doesn't evaluate the main() function). Example: \"\"\" file: ./src/helpers.pinc \"\"\" PROTOCONF_VERSION = \"0.1.3\" def format_name ( person ): # assumes `person` is a proto message that have `first_name` and `last_name` return \" %s %s \" % ( person . first_name , person . last_name ) We can now load the variables and functions in this file to another starlark file ( pinc , .pconf or .mpconf ) load ( \"//helpers.pinc\" , \"PROTOCONF_VERSION\" , \"format_name\" )","title":"Structure Your Code"},{"location":"structuring-your-code/#structuring-your-code","text":"Sometimes, when we use protoconf we will want to write helpers functions and global constants that we might want to include in multiple configs. We can define those in .pinc files. .pinc files are starlark code which doesn't produce configs (doesn't evaluate the main() function). Example: \"\"\" file: ./src/helpers.pinc \"\"\" PROTOCONF_VERSION = \"0.1.3\" def format_name ( person ): # assumes `person` is a proto message that have `first_name` and `last_name` return \" %s %s \" % ( person . first_name , person . last_name ) We can now load the variables and functions in this file to another starlark file ( pinc , .pconf or .mpconf ) load ( \"//helpers.pinc\" , \"PROTOCONF_VERSION\" , \"format_name\" )","title":"Structuring your code"},{"location":"integrations/terraform/","text":"Protoconf integration with Terraform Prerequists The terraform binray in your $PATH The protoconf binary in your $PATH Prepare Create a providers.tf file containing the providers declarations you need. provider \"random\" {} provider \"tls\" {} Initialize Terraform $ terraform init Generate the terraform protos $ protoconf import terraform Validate the outputs $ find src/terraform src/terraform src/terraform/v1 src/terraform/v1/terraform.proto src/terraform/v1/meta.proto src/terraform/tls src/terraform/tls/datasources src/terraform/tls/datasources/v3 src/terraform/tls/datasources/v3/public.proto src/terraform/tls/datasources/v3/certificate.proto src/terraform/tls/resources src/terraform/tls/resources/v3 src/terraform/tls/resources/v3/private.proto src/terraform/tls/resources/v3/self.proto src/terraform/tls/resources/v3/cert.proto src/terraform/tls/resources/v3/locally.proto src/terraform/tls/provider src/terraform/tls/provider/v3 src/terraform/tls/provider/v3/tls.proto src/terraform/random src/terraform/random/resources src/terraform/random/resources/v3 src/terraform/random/resources/v3/password.proto src/terraform/random/resources/v3/integer.proto src/terraform/random/resources/v3/string.proto src/terraform/random/resources/v3/pet.proto src/terraform/random/resources/v3/shuffle.proto src/terraform/random/resources/v3/id.proto src/terraform/random/resources/v3/uuid.proto src/terraform/random/provider src/terraform/random/provider/v3 src/terraform/random/provider/v3/random.proto The src/terraform/terraform.proto should looks like this: syntax = \"proto3\" ; package terraform . v1 ; import \"terraform/random/provider/v3/random.proto\" ; import \"terraform/random/resources/v3/id.proto\" ; import \"terraform/random/resources/v3/integer.proto\" ; import \"terraform/random/resources/v3/password.proto\" ; import \"terraform/random/resources/v3/pet.proto\" ; import \"terraform/random/resources/v3/shuffle.proto\" ; import \"terraform/random/resources/v3/string.proto\" ; import \"terraform/random/resources/v3/uuid.proto\" ; import \"terraform/tls/datasources/v3/certificate.proto\" ; import \"terraform/tls/datasources/v3/public.proto\" ; import \"terraform/tls/provider/v3/tls.proto\" ; import \"terraform/tls/resources/v3/cert.proto\" ; import \"terraform/tls/resources/v3/locally.proto\" ; import \"terraform/tls/resources/v3/private.proto\" ; import \"terraform/tls/resources/v3/self.proto\" ; message Terraform { Resources resource = 1 ; Datasources data = 2 ; Providers provider = 3 ; map < string , Variable > variable = 4 ; map < string , Output > output = 5 ; map < string , string > locals = 6 ; Module module = 7 ; TerraformSettings terraform = 8 ; message Resources { map < string , terraform.random.resources.v3.RandomId > random_id = 1 [ json_name = \"random_id\" ]; map < string , terraform.random.resources.v3.RandomInteger > random_integer = 2 [ json_name = \"random_integer\" ]; map < string , terraform.random.resources.v3.RandomPassword > random_password = 3 [ json_name = \"random_password\" ]; map < string , terraform.random.resources.v3.RandomPet > random_pet = 4 [ json_name = \"random_pet\" ]; map < string , terraform.random.resources.v3.RandomShuffle > random_shuffle = 5 [ json_name = \"random_shuffle\" ]; map < string , terraform.random.resources.v3.RandomString > random_string = 6 [ json_name = \"random_string\" ]; map < string , terraform.random.resources.v3.RandomUuid > random_uuid = 7 [ json_name = \"random_uuid\" ]; map < string , terraform.tls.resources.v3.TlsCertRequest > tls_cert_request = 8 [ json_name = \"tls_cert_request\" ]; map < string , terraform.tls.resources.v3.TlsLocallySignedCert > tls_locally_signed_cert = 9 [ json_name = \"tls_locally_signed_cert\" ]; map < string , terraform.tls.resources.v3.TlsPrivateKey > tls_private_key = 10 [ json_name = \"tls_private_key\" ]; map < string , terraform.tls.resources.v3.TlsSelfSignedCert > tls_self_signed_cert = 11 [ json_name = \"tls_self_signed_cert\" ]; } message Datasources { map < string , terraform.tls.datasources.v3.TlsCertificate > tls_certificate = 1 [ json_name = \"tls_certificate\" ]; map < string , terraform.tls.datasources.v3.TlsPublicKey > tls_public_key = 2 [ json_name = \"tls_public_key\" ]; } message Providers { repeated terraform.random.provider.v3.Random random = 1 ; repeated terraform.tls.provider.v3.Tls tls = 2 ; } message Variable { string type = 1 ; string description = 2 ; string default = 3 ; } message Output { string value = 1 ; } message Module { } message TerraformSettings { string required_version = 1 [ json_name = \"required_version\" ]; map < string , Provider > required_providers = 2 [ json_name = \"required_providers\" ]; Backend backend = 3 ; message Provider { string source = 1 ; string version = 2 ; } message Backend { oneof config { BackendLocal local = 1 ; BackendRemote remote = 2 ; BackendS3 s3 = 3 ; } message BackendLocal { string path = 1 ; string workspace_dir = 2 [ json_name = \"workspace_dir\" ]; } message BackendRemote { //(Optional) The remote backend hostname to connect to. Defaults to app.terraform.io. string hostname = 1 ; //(Required) The name of the organization containing the targeted workspace(s). string organization = 2 ; //(Optional) The token used to authenticate with the remote backend. We recommend omitting the token from the configuration, and instead using `terraform login` or manually configuring `credentials` in the CLI config file. string token = 3 ; //(Required) A block specifying which remote workspace(s) to use. The workspaces block supports the following keys Workspace workspaces = 4 ; message Workspace { //(Optional) The full name of one remote workspace. When configured, only the default workspace can be used. This option conflicts with prefix. string name = 1 ; //(Optional) A prefix used in the names of one or more remote workspaces, all of which can be used with this configuration. The full workspace names are used in Terraform Cloud, and the short names (minus the prefix) are used on the command line for Terraform CLI workspaces. If omitted, only the default workspace can be used. This option conflicts with name. string prefix = 2 ; } } message BackendS3 { string region = 1 ; string access_key = 2 [ json_name = \"access_key\" ]; string secret_key = 3 [ json_name = \"secret_key\" ]; string iam_endpoint = 4 [ json_name = \"iam_endpoint\" ]; string max_retries = 5 [ json_name = \"max_retries\" ]; string profile = 6 ; string shared_credentials_file = 7 [ json_name = \"shared_credentials_file\" ]; string skip_credentials_validation = 8 [ json_name = \"skip_credentials_validation\" ]; string skip_region_validation = 9 [ json_name = \"skip_region_validation\" ]; string skip_metadata_api_check = 10 [ json_name = \"skip_metadata_api_check\" ]; string sts_endpoint = 11 [ json_name = \"sts_endpoint\" ]; string token = 12 ; string assume_role_duration_seconds = 13 [ json_name = \"assume_role_duration_seconds\" ]; string assume_role_policy = 14 [ json_name = \"assume_role_policy\" ]; string assume_role_policy_arns = 15 [ json_name = \"assume_role_policy_arns\" ]; string assume_role_tags = 16 [ json_name = \"assume_role_tags\" ]; string assume_role_transitive_tag_keys = 17 [ json_name = \"assume_role_transitive_tag_keys\" ]; string external_id = 18 [ json_name = \"external_id\" ]; string role_arn = 19 [ json_name = \"role_arn\" ]; string session_name = 20 [ json_name = \"session_name\" ]; string bucket = 21 ; string key = 22 ; string acl = 23 ; string encrypt = 24 ; string endpoint = 25 ; string force_path_style = 26 [ json_name = \"force_path_style\" ]; string kms_key_id = 27 [ json_name = \"kms_key_id\" ]; string sse_customer_key = 28 [ json_name = \"sse_customer_key\" ]; string workspace_key_prefix = 29 [ json_name = \"workspace_key_prefix\" ]; string dynamodb_endpoint = 30 [ json_name = \"dynamodb_endpoint\" ]; string dynamodb_table = 31 [ json_name = \"dynamodb_table\" ]; } } } } Create a .tf.pconf file # vim: filetype=python # ./src/tfdemo/tfdemo.tf.pconf load ( \"//terraform/v1/terraform.proto\" , \"Terraform\" ) load ( \"//terraform/random/provider/v3/random.proto\" , \"Random\" ) load ( \"//terraform/random/resources/v3/pet.proto\" , \"RandomPet\" ) tf = Terraform ( provider = Terraform . Providers ( random = [ Random ()]), resource = Terraform . Resources (), output = {}, ) tf . resource . random_pet [ \"my_dog_name\" ] = RandomPet () tf . output [ \"my_dog_name\" ] = Terraform . Output ( value = \"$ {random_pet.my_dog_name.id} \" ) def main (): return tf compile the config $ protoconf compile . Check the output cat materialized_config/tfdemo/tfdemo.tf.materialized_JSON { \"protoFile\" : \"terraform/terraform.proto\" , \"value\" : { \"@type\" : \"type.googleapis.com/terraform.Terraform\" , \"output\" : { \"my_dog_name\" : { \"value\" : \"${random_pet.my_dog_name.id}\" } }, \"provider\" : { \"random\" : [{}] }, \"resource\" : { \"random_pet\" : { \"my_dog_name\" : {} } } } } prepare to run terraform $ mkdir tf $ cat materialized_config/tfdemo/tfdemo.tf.materialized_JSON | jq '.value | del(.[\"@type\"])' > tf/test.tf.json $ terraform -chdir = tf init $ terraform -chdir = tf plan An execution plan has been generated and is shown below. Resource actions are indicated with the following symbols: + create Terraform will perform the following actions: # random_pet.my_dog_name will be created + resource \"random_pet\" \"my_dog_name\" { + id = ( known after apply ) + length = 2 + separator = \"-\" } Plan: 1 to add, 0 to change, 0 to destroy. Changes to Outputs: + my_dog_name = ( known after apply ) ------------------------------------------------------------------------ Note: You didn 't specify an \"-out\" parameter to save this plan, so Terraform can' t guarantee that exactly these actions will be performed if \"terraform apply\" is subsequently run. $ terraform -chdir = tf apply -auto-approve random_pet.my_dog_name: Creating... random_pet.my_dog_name: Creation complete after 0s [ id = key-zebra ] Apply complete! Resources: 1 added, 0 changed, 0 destroyed. Outputs: my_dog_name = \"key-zebra\"","title":"Terraform"},{"location":"integrations/terraform/#protoconf-integration-with-terraform","text":"","title":"Protoconf integration with Terraform"},{"location":"integrations/terraform/#prerequists","text":"The terraform binray in your $PATH The protoconf binary in your $PATH","title":"Prerequists"},{"location":"integrations/terraform/#prepare","text":"Create a providers.tf file containing the providers declarations you need. provider \"random\" {} provider \"tls\" {}","title":"Prepare"},{"location":"integrations/terraform/#initialize-terraform","text":"$ terraform init","title":"Initialize Terraform"},{"location":"integrations/terraform/#generate-the-terraform-protos","text":"$ protoconf import terraform Validate the outputs $ find src/terraform src/terraform src/terraform/v1 src/terraform/v1/terraform.proto src/terraform/v1/meta.proto src/terraform/tls src/terraform/tls/datasources src/terraform/tls/datasources/v3 src/terraform/tls/datasources/v3/public.proto src/terraform/tls/datasources/v3/certificate.proto src/terraform/tls/resources src/terraform/tls/resources/v3 src/terraform/tls/resources/v3/private.proto src/terraform/tls/resources/v3/self.proto src/terraform/tls/resources/v3/cert.proto src/terraform/tls/resources/v3/locally.proto src/terraform/tls/provider src/terraform/tls/provider/v3 src/terraform/tls/provider/v3/tls.proto src/terraform/random src/terraform/random/resources src/terraform/random/resources/v3 src/terraform/random/resources/v3/password.proto src/terraform/random/resources/v3/integer.proto src/terraform/random/resources/v3/string.proto src/terraform/random/resources/v3/pet.proto src/terraform/random/resources/v3/shuffle.proto src/terraform/random/resources/v3/id.proto src/terraform/random/resources/v3/uuid.proto src/terraform/random/provider src/terraform/random/provider/v3 src/terraform/random/provider/v3/random.proto The src/terraform/terraform.proto should looks like this: syntax = \"proto3\" ; package terraform . v1 ; import \"terraform/random/provider/v3/random.proto\" ; import \"terraform/random/resources/v3/id.proto\" ; import \"terraform/random/resources/v3/integer.proto\" ; import \"terraform/random/resources/v3/password.proto\" ; import \"terraform/random/resources/v3/pet.proto\" ; import \"terraform/random/resources/v3/shuffle.proto\" ; import \"terraform/random/resources/v3/string.proto\" ; import \"terraform/random/resources/v3/uuid.proto\" ; import \"terraform/tls/datasources/v3/certificate.proto\" ; import \"terraform/tls/datasources/v3/public.proto\" ; import \"terraform/tls/provider/v3/tls.proto\" ; import \"terraform/tls/resources/v3/cert.proto\" ; import \"terraform/tls/resources/v3/locally.proto\" ; import \"terraform/tls/resources/v3/private.proto\" ; import \"terraform/tls/resources/v3/self.proto\" ; message Terraform { Resources resource = 1 ; Datasources data = 2 ; Providers provider = 3 ; map < string , Variable > variable = 4 ; map < string , Output > output = 5 ; map < string , string > locals = 6 ; Module module = 7 ; TerraformSettings terraform = 8 ; message Resources { map < string , terraform.random.resources.v3.RandomId > random_id = 1 [ json_name = \"random_id\" ]; map < string , terraform.random.resources.v3.RandomInteger > random_integer = 2 [ json_name = \"random_integer\" ]; map < string , terraform.random.resources.v3.RandomPassword > random_password = 3 [ json_name = \"random_password\" ]; map < string , terraform.random.resources.v3.RandomPet > random_pet = 4 [ json_name = \"random_pet\" ]; map < string , terraform.random.resources.v3.RandomShuffle > random_shuffle = 5 [ json_name = \"random_shuffle\" ]; map < string , terraform.random.resources.v3.RandomString > random_string = 6 [ json_name = \"random_string\" ]; map < string , terraform.random.resources.v3.RandomUuid > random_uuid = 7 [ json_name = \"random_uuid\" ]; map < string , terraform.tls.resources.v3.TlsCertRequest > tls_cert_request = 8 [ json_name = \"tls_cert_request\" ]; map < string , terraform.tls.resources.v3.TlsLocallySignedCert > tls_locally_signed_cert = 9 [ json_name = \"tls_locally_signed_cert\" ]; map < string , terraform.tls.resources.v3.TlsPrivateKey > tls_private_key = 10 [ json_name = \"tls_private_key\" ]; map < string , terraform.tls.resources.v3.TlsSelfSignedCert > tls_self_signed_cert = 11 [ json_name = \"tls_self_signed_cert\" ]; } message Datasources { map < string , terraform.tls.datasources.v3.TlsCertificate > tls_certificate = 1 [ json_name = \"tls_certificate\" ]; map < string , terraform.tls.datasources.v3.TlsPublicKey > tls_public_key = 2 [ json_name = \"tls_public_key\" ]; } message Providers { repeated terraform.random.provider.v3.Random random = 1 ; repeated terraform.tls.provider.v3.Tls tls = 2 ; } message Variable { string type = 1 ; string description = 2 ; string default = 3 ; } message Output { string value = 1 ; } message Module { } message TerraformSettings { string required_version = 1 [ json_name = \"required_version\" ]; map < string , Provider > required_providers = 2 [ json_name = \"required_providers\" ]; Backend backend = 3 ; message Provider { string source = 1 ; string version = 2 ; } message Backend { oneof config { BackendLocal local = 1 ; BackendRemote remote = 2 ; BackendS3 s3 = 3 ; } message BackendLocal { string path = 1 ; string workspace_dir = 2 [ json_name = \"workspace_dir\" ]; } message BackendRemote { //(Optional) The remote backend hostname to connect to. Defaults to app.terraform.io. string hostname = 1 ; //(Required) The name of the organization containing the targeted workspace(s). string organization = 2 ; //(Optional) The token used to authenticate with the remote backend. We recommend omitting the token from the configuration, and instead using `terraform login` or manually configuring `credentials` in the CLI config file. string token = 3 ; //(Required) A block specifying which remote workspace(s) to use. The workspaces block supports the following keys Workspace workspaces = 4 ; message Workspace { //(Optional) The full name of one remote workspace. When configured, only the default workspace can be used. This option conflicts with prefix. string name = 1 ; //(Optional) A prefix used in the names of one or more remote workspaces, all of which can be used with this configuration. The full workspace names are used in Terraform Cloud, and the short names (minus the prefix) are used on the command line for Terraform CLI workspaces. If omitted, only the default workspace can be used. This option conflicts with name. string prefix = 2 ; } } message BackendS3 { string region = 1 ; string access_key = 2 [ json_name = \"access_key\" ]; string secret_key = 3 [ json_name = \"secret_key\" ]; string iam_endpoint = 4 [ json_name = \"iam_endpoint\" ]; string max_retries = 5 [ json_name = \"max_retries\" ]; string profile = 6 ; string shared_credentials_file = 7 [ json_name = \"shared_credentials_file\" ]; string skip_credentials_validation = 8 [ json_name = \"skip_credentials_validation\" ]; string skip_region_validation = 9 [ json_name = \"skip_region_validation\" ]; string skip_metadata_api_check = 10 [ json_name = \"skip_metadata_api_check\" ]; string sts_endpoint = 11 [ json_name = \"sts_endpoint\" ]; string token = 12 ; string assume_role_duration_seconds = 13 [ json_name = \"assume_role_duration_seconds\" ]; string assume_role_policy = 14 [ json_name = \"assume_role_policy\" ]; string assume_role_policy_arns = 15 [ json_name = \"assume_role_policy_arns\" ]; string assume_role_tags = 16 [ json_name = \"assume_role_tags\" ]; string assume_role_transitive_tag_keys = 17 [ json_name = \"assume_role_transitive_tag_keys\" ]; string external_id = 18 [ json_name = \"external_id\" ]; string role_arn = 19 [ json_name = \"role_arn\" ]; string session_name = 20 [ json_name = \"session_name\" ]; string bucket = 21 ; string key = 22 ; string acl = 23 ; string encrypt = 24 ; string endpoint = 25 ; string force_path_style = 26 [ json_name = \"force_path_style\" ]; string kms_key_id = 27 [ json_name = \"kms_key_id\" ]; string sse_customer_key = 28 [ json_name = \"sse_customer_key\" ]; string workspace_key_prefix = 29 [ json_name = \"workspace_key_prefix\" ]; string dynamodb_endpoint = 30 [ json_name = \"dynamodb_endpoint\" ]; string dynamodb_table = 31 [ json_name = \"dynamodb_table\" ]; } } } }","title":"Generate the terraform protos"},{"location":"integrations/terraform/#create-a-tfpconf-file","text":"# vim: filetype=python # ./src/tfdemo/tfdemo.tf.pconf load ( \"//terraform/v1/terraform.proto\" , \"Terraform\" ) load ( \"//terraform/random/provider/v3/random.proto\" , \"Random\" ) load ( \"//terraform/random/resources/v3/pet.proto\" , \"RandomPet\" ) tf = Terraform ( provider = Terraform . Providers ( random = [ Random ()]), resource = Terraform . Resources (), output = {}, ) tf . resource . random_pet [ \"my_dog_name\" ] = RandomPet () tf . output [ \"my_dog_name\" ] = Terraform . Output ( value = \"$ {random_pet.my_dog_name.id} \" ) def main (): return tf","title":"Create a .tf.pconf file"},{"location":"integrations/terraform/#compile-the-config","text":"$ protoconf compile . Check the output cat materialized_config/tfdemo/tfdemo.tf.materialized_JSON { \"protoFile\" : \"terraform/terraform.proto\" , \"value\" : { \"@type\" : \"type.googleapis.com/terraform.Terraform\" , \"output\" : { \"my_dog_name\" : { \"value\" : \"${random_pet.my_dog_name.id}\" } }, \"provider\" : { \"random\" : [{}] }, \"resource\" : { \"random_pet\" : { \"my_dog_name\" : {} } } } }","title":"compile the config"},{"location":"integrations/terraform/#prepare-to-run-terraform","text":"$ mkdir tf $ cat materialized_config/tfdemo/tfdemo.tf.materialized_JSON | jq '.value | del(.[\"@type\"])' > tf/test.tf.json $ terraform -chdir = tf init $ terraform -chdir = tf plan An execution plan has been generated and is shown below. Resource actions are indicated with the following symbols: + create Terraform will perform the following actions: # random_pet.my_dog_name will be created + resource \"random_pet\" \"my_dog_name\" { + id = ( known after apply ) + length = 2 + separator = \"-\" } Plan: 1 to add, 0 to change, 0 to destroy. Changes to Outputs: + my_dog_name = ( known after apply ) ------------------------------------------------------------------------ Note: You didn 't specify an \"-out\" parameter to save this plan, so Terraform can' t guarantee that exactly these actions will be performed if \"terraform apply\" is subsequently run. $ terraform -chdir = tf apply -auto-approve random_pet.my_dog_name: Creating... random_pet.my_dog_name: Creation complete after 0s [ id = key-zebra ] Apply complete! Resources: 1 added, 0 changed, 0 destroyed. Outputs: my_dog_name = \"key-zebra\"","title":"prepare to run terraform"},{"location":"integrations/terraform_kubernetes/","text":"Protoconf integration with Terraform for managing Kubernetes resources Prerequisites The terraform binary in your $PATH The protoconf binary in your $PATH Prepare Create a providers.tf file containing the providers declarations you need. provider \"kubernetes\" {} Initialize Terraform $ terraform init Generate the terraform protos $ protoconf import terraform Validate the outpus $ find src/terraform src/terraform src/terraform/v1 src/terraform/v1/terraform.proto src/terraform/v1/meta.proto src/terraform/kubernetes src/terraform/kubernetes/datasources src/terraform/kubernetes/datasources/v2 src/terraform/kubernetes/datasources/v2/namespace.proto src/terraform/kubernetes/datasources/v2/all.proto src/terraform/kubernetes/datasources/v2/persistent.proto src/terraform/kubernetes/datasources/v2/storage.proto src/terraform/kubernetes/datasources/v2/service.proto src/terraform/kubernetes/datasources/v2/ingress.proto src/terraform/kubernetes/datasources/v2/config.proto src/terraform/kubernetes/datasources/v2/pod.proto src/terraform/kubernetes/datasources/v2/secret.proto src/terraform/kubernetes/resources src/terraform/kubernetes/resources/v2 src/terraform/kubernetes/resources/v2/mutating.proto src/terraform/kubernetes/resources/v2/priority.proto src/terraform/kubernetes/resources/v2/namespace.proto src/terraform/kubernetes/resources/v2/validating.proto src/terraform/kubernetes/resources/v2/stateful.proto src/terraform/kubernetes/resources/v2/manifest.proto src/terraform/kubernetes/resources/v2/cluster.proto src/terraform/kubernetes/resources/v2/api.proto src/terraform/kubernetes/resources/v2/job.proto src/terraform/kubernetes/resources/v2/persistent.proto src/terraform/kubernetes/resources/v2/daemonset.proto src/terraform/kubernetes/resources/v2/cron.proto src/terraform/kubernetes/resources/v2/role.proto src/terraform/kubernetes/resources/v2/deployment.proto src/terraform/kubernetes/resources/v2/storage.proto src/terraform/kubernetes/resources/v2/csi.proto src/terraform/kubernetes/resources/v2/endpoints.proto src/terraform/kubernetes/resources/v2/service.proto src/terraform/kubernetes/resources/v2/ingress.proto src/terraform/kubernetes/resources/v2/default.proto src/terraform/kubernetes/resources/v2/certificate.proto src/terraform/kubernetes/resources/v2/replication.proto src/terraform/kubernetes/resources/v2/limit.proto src/terraform/kubernetes/resources/v2/horizontal.proto src/terraform/kubernetes/resources/v2/resource.proto src/terraform/kubernetes/resources/v2/network.proto src/terraform/kubernetes/resources/v2/config.proto src/terraform/kubernetes/resources/v2/daemon.proto src/terraform/kubernetes/resources/v2/pod.proto src/terraform/kubernetes/resources/v2/secret.proto src/terraform/kubernetes/provider src/terraform/kubernetes/provider/v2 src/terraform/kubernetes/provider/v2/kubernetes.proto Create directory for the Starlark configuration file (.pconf) $ mkdir src/proto-kube/ Create the Starlark configuration file (.pconf) # vim: filetype=python # ./src/proto-kube/kube-pod.pconf load ( \"//terraform/v1/terraform.proto\" , \"Terraform\" ) load ( \"//terraform/kubernetes/provider/v2/kubernetes.proto\" , \"Kubernetes\" ) load ( \"//terraform/kubernetes/resources/v2/pod.proto\" , \"KubernetesPod\" ) tf = Terraform ( provider = Terraform . Providers ( kubernetes = [ Kubernetes ( config_path = \"/path/to/kubeconfig\" )] ), resource = Terraform . Resources (), output = {}, ) name = KubernetesPod . Metadata ( name = \"example-pod\" ) spec = KubernetesPod . Spec ( container = [ KubernetesPod . Spec . Container ( name = \"test-container\" , image = \"centos/tools\" , command = [ \"/bin/bash\" , \"-c\" , \"sleep 2000000000000\" ], )] ) tf . resource . kubernetes_pod [ \"my_pod\" ] = KubernetesPod ( metadata = name , spec = spec ) def main (): return tf Compile the config $ protoconf compile . Check the json output cat materialized_config/proto-kube/kube-pod.materialized_JSON { \"protoFile\" : \"terraform/terraform.proto\" , \"value\" : { \"@type\" : \"type.googleapis.com/terraform.Terraform\" , \"provider\" : { \"kubernetes\" : [ { \"config_path\" : \"/path/to/kubeconfig\" } ] }, \"resource\" : { \"kubernetes_pod\" : { \"my_pod\" : { \"metadata\" : { \"name\" : \"example-pod\" }, \"spec\" : { \"container\" : { \"command\" : [ \"/bin/bash\" , \"-c\" , \"sleep 2000000000000\" ], \"image\" : \"centos/tools\" , \"name\" : \"test-container\" } } } } } } } Prepare to run Terraform Create Terraform working directory $ mkdir tf Process json required by Terraform $ cat materialized_config/proto-kube/kube-pod.materialized_JSON | \\ jq '.value | del(.[\"@type\"])' > tf/proto-kube.tf.json Check the json required by Terraform $ cat tf/proto-kube.tf.json { \"provider\" : { \"kubernetes\" : [ { \"config_path\" : \"./kubeconfig\" } ] } , \"resource\" : { \"kubernetes_pod\" : { \"my_pod\" : { \"metadata\" : { \"name\" : \"example-pod\" } , \"spec\" : { \"container\" : { \"command\" : [ \"/bin/bash\" , \"-c\" , \"sleep 2000000000000\" ] , \"image\" : \"centos/tools\" , \"name\" : \"test-container\" } } } } } } Run Terraform init $ cd tf ~/tf $ terraform init Initializing the backend... Initializing provider plugins... - Finding latest version of hashicorp/kubernetes... - Installing hashicorp/kubernetes v2.3.2... - Installed hashicorp/kubernetes v2.3.2 ( signed by HashiCorp ) Terraform has created a lock file .terraform.lock.hcl to record the provider selections it made above. Include this file in your version control repository so that Terraform can guarantee to make the same selections by default when you run \"terraform init\" in the future. Terraform has been successfully initialized! You may now begin working with Terraform. Try running \"terraform plan\" to see any changes that are required for your infrastructure. All Terraform commands should now work. If you ever set or change modules or backend configuration for Terraform, rerun this command to reinitialize your working directory. If you forget, other commands will detect it and remind you to do so if necessary. Run Terraform plan ~/tf $ terraform plan Terraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols: + create [ ... ] Plan: 1 to add, 0 to change, 0 to destroy. \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Run Terraform apply ~/tf $ terraform apply -auto-approve Terraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols: + create Terraform will perform the following actions: # kubernetes_pod.my_pod will be created + resource \"kubernetes_pod\" \"my_pod\" { + id = ( known after apply ) + metadata { + generation = ( known after apply ) + name = \"example-pod\" + namespace = \"default\" + resource_version = ( known after apply ) + uid = ( known after apply ) } + spec { + automount_service_account_token = true + dns_policy = \"ClusterFirst\" + enable_service_links = true + host_ipc = false + host_network = false + host_pid = false + hostname = ( known after apply ) + node_name = ( known after apply ) + restart_policy = \"Always\" + service_account_name = ( known after apply ) + share_process_namespace = false + termination_grace_period_seconds = 30 + container { + command = [ + \"/bin/bash\" , + \"-c\" , + \"sleep 2000000000000\" , ] + image = \"centos/tools\" + image_pull_policy = ( known after apply ) + name = \"test-container\" + stdin = false + stdin_once = false + termination_message_path = \"/dev/termination-log\" + termination_message_policy = ( known after apply ) + tty = false + resources { + limits = ( known after apply ) + requests = ( known after apply ) } } [ ... ] Plan: 1 to add, 0 to change, 0 to destroy. kubernetes_pod.my_pod: Creating... kubernetes_pod.my_pod: Still creating... [ 10s elapsed ] kubernetes_pod.my_pod: Still creating... [ 20s elapsed ] kubernetes_pod.my_pod: Still creating... [ 30s elapsed ] kubernetes_pod.my_pod: Still creating... [ 40s elapsed ] kubernetes_pod.my_pod: Still creating... [ 50s elapsed ] kubernetes_pod.my_pod: Still creating... [ 1m0s elapsed ] kubernetes_pod.my_pod: Still creating... [ 1m10s elapsed ] kubernetes_pod.my_pod: Still creating... [ 1m20s elapsed ] kubernetes_pod.my_pod: Still creating... [ 1m30s elapsed ] kubernetes_pod.my_pod: Creation complete after 1m34s [ id = default/example-pod ] Apply complete! Resources: 1 added, 0 changed, 0 destroyed.","title":"Terraform Kubernetes"},{"location":"integrations/terraform_kubernetes/#protoconf-integration-with-terraform-for-managing-kubernetes-resources","text":"","title":"Protoconf integration with Terraform for managing Kubernetes resources"},{"location":"integrations/terraform_kubernetes/#prerequisites","text":"The terraform binary in your $PATH The protoconf binary in your $PATH","title":"Prerequisites"},{"location":"integrations/terraform_kubernetes/#prepare","text":"Create a providers.tf file containing the providers declarations you need. provider \"kubernetes\" {}","title":"Prepare"},{"location":"integrations/terraform_kubernetes/#initialize-terraform","text":"$ terraform init","title":"Initialize Terraform"},{"location":"integrations/terraform_kubernetes/#generate-the-terraform-protos","text":"$ protoconf import terraform Validate the outpus $ find src/terraform src/terraform src/terraform/v1 src/terraform/v1/terraform.proto src/terraform/v1/meta.proto src/terraform/kubernetes src/terraform/kubernetes/datasources src/terraform/kubernetes/datasources/v2 src/terraform/kubernetes/datasources/v2/namespace.proto src/terraform/kubernetes/datasources/v2/all.proto src/terraform/kubernetes/datasources/v2/persistent.proto src/terraform/kubernetes/datasources/v2/storage.proto src/terraform/kubernetes/datasources/v2/service.proto src/terraform/kubernetes/datasources/v2/ingress.proto src/terraform/kubernetes/datasources/v2/config.proto src/terraform/kubernetes/datasources/v2/pod.proto src/terraform/kubernetes/datasources/v2/secret.proto src/terraform/kubernetes/resources src/terraform/kubernetes/resources/v2 src/terraform/kubernetes/resources/v2/mutating.proto src/terraform/kubernetes/resources/v2/priority.proto src/terraform/kubernetes/resources/v2/namespace.proto src/terraform/kubernetes/resources/v2/validating.proto src/terraform/kubernetes/resources/v2/stateful.proto src/terraform/kubernetes/resources/v2/manifest.proto src/terraform/kubernetes/resources/v2/cluster.proto src/terraform/kubernetes/resources/v2/api.proto src/terraform/kubernetes/resources/v2/job.proto src/terraform/kubernetes/resources/v2/persistent.proto src/terraform/kubernetes/resources/v2/daemonset.proto src/terraform/kubernetes/resources/v2/cron.proto src/terraform/kubernetes/resources/v2/role.proto src/terraform/kubernetes/resources/v2/deployment.proto src/terraform/kubernetes/resources/v2/storage.proto src/terraform/kubernetes/resources/v2/csi.proto src/terraform/kubernetes/resources/v2/endpoints.proto src/terraform/kubernetes/resources/v2/service.proto src/terraform/kubernetes/resources/v2/ingress.proto src/terraform/kubernetes/resources/v2/default.proto src/terraform/kubernetes/resources/v2/certificate.proto src/terraform/kubernetes/resources/v2/replication.proto src/terraform/kubernetes/resources/v2/limit.proto src/terraform/kubernetes/resources/v2/horizontal.proto src/terraform/kubernetes/resources/v2/resource.proto src/terraform/kubernetes/resources/v2/network.proto src/terraform/kubernetes/resources/v2/config.proto src/terraform/kubernetes/resources/v2/daemon.proto src/terraform/kubernetes/resources/v2/pod.proto src/terraform/kubernetes/resources/v2/secret.proto src/terraform/kubernetes/provider src/terraform/kubernetes/provider/v2 src/terraform/kubernetes/provider/v2/kubernetes.proto","title":"Generate the terraform protos"},{"location":"integrations/terraform_kubernetes/#create-directory-for-the-starlark-configuration-file-pconf","text":"$ mkdir src/proto-kube/","title":"Create directory for the Starlark configuration file (.pconf)"},{"location":"integrations/terraform_kubernetes/#create-the-starlark-configuration-file-pconf","text":"# vim: filetype=python # ./src/proto-kube/kube-pod.pconf load ( \"//terraform/v1/terraform.proto\" , \"Terraform\" ) load ( \"//terraform/kubernetes/provider/v2/kubernetes.proto\" , \"Kubernetes\" ) load ( \"//terraform/kubernetes/resources/v2/pod.proto\" , \"KubernetesPod\" ) tf = Terraform ( provider = Terraform . Providers ( kubernetes = [ Kubernetes ( config_path = \"/path/to/kubeconfig\" )] ), resource = Terraform . Resources (), output = {}, ) name = KubernetesPod . Metadata ( name = \"example-pod\" ) spec = KubernetesPod . Spec ( container = [ KubernetesPod . Spec . Container ( name = \"test-container\" , image = \"centos/tools\" , command = [ \"/bin/bash\" , \"-c\" , \"sleep 2000000000000\" ], )] ) tf . resource . kubernetes_pod [ \"my_pod\" ] = KubernetesPod ( metadata = name , spec = spec ) def main (): return tf","title":"Create the Starlark configuration file (.pconf)"},{"location":"integrations/terraform_kubernetes/#compile-the-config","text":"$ protoconf compile .","title":"Compile the config"},{"location":"integrations/terraform_kubernetes/#check-the-json-output","text":"cat materialized_config/proto-kube/kube-pod.materialized_JSON { \"protoFile\" : \"terraform/terraform.proto\" , \"value\" : { \"@type\" : \"type.googleapis.com/terraform.Terraform\" , \"provider\" : { \"kubernetes\" : [ { \"config_path\" : \"/path/to/kubeconfig\" } ] }, \"resource\" : { \"kubernetes_pod\" : { \"my_pod\" : { \"metadata\" : { \"name\" : \"example-pod\" }, \"spec\" : { \"container\" : { \"command\" : [ \"/bin/bash\" , \"-c\" , \"sleep 2000000000000\" ], \"image\" : \"centos/tools\" , \"name\" : \"test-container\" } } } } } } }","title":"Check the json output"},{"location":"integrations/terraform_kubernetes/#prepare-to-run-terraform","text":"","title":"Prepare to run Terraform"},{"location":"integrations/terraform_kubernetes/#create-terraform-working-directory","text":"$ mkdir tf","title":"Create Terraform working directory"},{"location":"integrations/terraform_kubernetes/#process-json-required-by-terraform","text":"$ cat materialized_config/proto-kube/kube-pod.materialized_JSON | \\ jq '.value | del(.[\"@type\"])' > tf/proto-kube.tf.json","title":"Process json required by Terraform"},{"location":"integrations/terraform_kubernetes/#check-the-json-required-by-terraform","text":"$ cat tf/proto-kube.tf.json { \"provider\" : { \"kubernetes\" : [ { \"config_path\" : \"./kubeconfig\" } ] } , \"resource\" : { \"kubernetes_pod\" : { \"my_pod\" : { \"metadata\" : { \"name\" : \"example-pod\" } , \"spec\" : { \"container\" : { \"command\" : [ \"/bin/bash\" , \"-c\" , \"sleep 2000000000000\" ] , \"image\" : \"centos/tools\" , \"name\" : \"test-container\" } } } } } }","title":"Check the json required by Terraform"},{"location":"integrations/terraform_kubernetes/#run-terraform-init","text":"$ cd tf ~/tf $ terraform init Initializing the backend... Initializing provider plugins... - Finding latest version of hashicorp/kubernetes... - Installing hashicorp/kubernetes v2.3.2... - Installed hashicorp/kubernetes v2.3.2 ( signed by HashiCorp ) Terraform has created a lock file .terraform.lock.hcl to record the provider selections it made above. Include this file in your version control repository so that Terraform can guarantee to make the same selections by default when you run \"terraform init\" in the future. Terraform has been successfully initialized! You may now begin working with Terraform. Try running \"terraform plan\" to see any changes that are required for your infrastructure. All Terraform commands should now work. If you ever set or change modules or backend configuration for Terraform, rerun this command to reinitialize your working directory. If you forget, other commands will detect it and remind you to do so if necessary.","title":"Run Terraform init"},{"location":"integrations/terraform_kubernetes/#run-terraform-plan","text":"~/tf $ terraform plan Terraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols: + create [ ... ] Plan: 1 to add, 0 to change, 0 to destroy. \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500","title":"Run Terraform plan"},{"location":"integrations/terraform_kubernetes/#run-terraform-apply","text":"~/tf $ terraform apply -auto-approve Terraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols: + create Terraform will perform the following actions: # kubernetes_pod.my_pod will be created + resource \"kubernetes_pod\" \"my_pod\" { + id = ( known after apply ) + metadata { + generation = ( known after apply ) + name = \"example-pod\" + namespace = \"default\" + resource_version = ( known after apply ) + uid = ( known after apply ) } + spec { + automount_service_account_token = true + dns_policy = \"ClusterFirst\" + enable_service_links = true + host_ipc = false + host_network = false + host_pid = false + hostname = ( known after apply ) + node_name = ( known after apply ) + restart_policy = \"Always\" + service_account_name = ( known after apply ) + share_process_namespace = false + termination_grace_period_seconds = 30 + container { + command = [ + \"/bin/bash\" , + \"-c\" , + \"sleep 2000000000000\" , ] + image = \"centos/tools\" + image_pull_policy = ( known after apply ) + name = \"test-container\" + stdin = false + stdin_once = false + termination_message_path = \"/dev/termination-log\" + termination_message_policy = ( known after apply ) + tty = false + resources { + limits = ( known after apply ) + requests = ( known after apply ) } } [ ... ] Plan: 1 to add, 0 to change, 0 to destroy. kubernetes_pod.my_pod: Creating... kubernetes_pod.my_pod: Still creating... [ 10s elapsed ] kubernetes_pod.my_pod: Still creating... [ 20s elapsed ] kubernetes_pod.my_pod: Still creating... [ 30s elapsed ] kubernetes_pod.my_pod: Still creating... [ 40s elapsed ] kubernetes_pod.my_pod: Still creating... [ 50s elapsed ] kubernetes_pod.my_pod: Still creating... [ 1m0s elapsed ] kubernetes_pod.my_pod: Still creating... [ 1m10s elapsed ] kubernetes_pod.my_pod: Still creating... [ 1m20s elapsed ] kubernetes_pod.my_pod: Still creating... [ 1m30s elapsed ] kubernetes_pod.my_pod: Creation complete after 1m34s [ id = default/example-pod ] Apply complete! Resources: 1 added, 0 changed, 0 destroyed.","title":"Run Terraform apply"}]}