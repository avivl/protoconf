{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction Modern services are comprised of many dynamic variables, that need to be changed regularly. Today, the process is unstructured and error prone. From ML model variables, kill switches, gradual rollout configuration, A/B experiment configuration and more - developers want their code to allow to be configured to the finer details. Protoconf is a modern approach to software configuration , inspired by Facebook's Configerator . Using Protoconf enables: Code review for configuration changes Enables the battle tested flow of pull-request & code-review. Configuration auditing out of the box (who did what, when?). The repository is the source of truth for the configuration deployed to production. No service restart required to pick up changes Instant delivery of configuration updates. Encourages writing software that doesn't know downtime. Clear representation of complex configuration Write configuration in Starlark (a Python dialect), no more copying & pasting from huge JSON files. Automated validation Config follows a fully-typed (Protobuf) schema. This allows writing validation code in Starlark, to verify your configuration before it is committed. What is protoconf Protoconf is a configuration management framework. We call it a framework because it provides a platform to manage the entire life cycle of configuration files (configs). Protoconf is a tool to aid in the specification and distribution of configs. The goals of specification are to be robust, composable, and less error-prone. The goals of distribution are to reach all of your machines quickly, reliably, and to be highly available even in disaster scenarios. When should you use Protoconf? You need to update and distribute configuration dynamically, often to a large number of hosts or services. You want to write your configuration with code. You need change history. You want to code review config changes. You want validation that config changes conform to a schema and do not violate invariants that you define. You want to canary (test) config changes before distributing them to production. You can tolerate eventual consistency; config updates are not atomic w.r.t. different consumers. You don't need config updates to propagate to your consumers within a certain SLA. Your configs are reasonably small You don't need very frequent config updates, more than about once every 5 mins. How Protoconf works Configuration update flow How this looks from the service's eyes This is roughly how configuration is consumed by a service. This paradigm encourages you to write software that can reconfigure itself in runtime rather than require a restart: Python #!/usr/bin/env python import grpc from v1.protoconf_service_pb2_grpc import ProtoconfServiceStub from v1.protoconf_service_pb2 import ConfigSubscriptionRequest from myproject.myconfig_pb2 import MyConfig channel = grpc . insecure_channel ( \"localhost:4300\" ) stub = ProtoconfServiceStub ( channel ) config = MyConfig () for update in stub . SubscribeForConfig ( ConfigSubscriptionRequest ( path = \"myproject/myconfig\" )): update . value . Unpack ( config ) print ( config ) As Protoconf uses Protobuf and gRPC, it supports delivering configuration to all major languages . See also: Protobuf overview .","title":"Home"},{"location":"#introduction","text":"Modern services are comprised of many dynamic variables, that need to be changed regularly. Today, the process is unstructured and error prone. From ML model variables, kill switches, gradual rollout configuration, A/B experiment configuration and more - developers want their code to allow to be configured to the finer details. Protoconf is a modern approach to software configuration , inspired by Facebook's Configerator . Using Protoconf enables: Code review for configuration changes Enables the battle tested flow of pull-request & code-review. Configuration auditing out of the box (who did what, when?). The repository is the source of truth for the configuration deployed to production. No service restart required to pick up changes Instant delivery of configuration updates. Encourages writing software that doesn't know downtime. Clear representation of complex configuration Write configuration in Starlark (a Python dialect), no more copying & pasting from huge JSON files. Automated validation Config follows a fully-typed (Protobuf) schema. This allows writing validation code in Starlark, to verify your configuration before it is committed.","title":"Introduction"},{"location":"#what-is-protoconf","text":"Protoconf is a configuration management framework. We call it a framework because it provides a platform to manage the entire life cycle of configuration files (configs). Protoconf is a tool to aid in the specification and distribution of configs. The goals of specification are to be robust, composable, and less error-prone. The goals of distribution are to reach all of your machines quickly, reliably, and to be highly available even in disaster scenarios.","title":"What is protoconf"},{"location":"#when-should-you-use-protoconf","text":"You need to update and distribute configuration dynamically, often to a large number of hosts or services. You want to write your configuration with code. You need change history. You want to code review config changes. You want validation that config changes conform to a schema and do not violate invariants that you define. You want to canary (test) config changes before distributing them to production. You can tolerate eventual consistency; config updates are not atomic w.r.t. different consumers. You don't need config updates to propagate to your consumers within a certain SLA. Your configs are reasonably small You don't need very frequent config updates, more than about once every 5 mins.","title":"When should you use Protoconf?"},{"location":"#how-protoconf-works","text":"","title":"How Protoconf works"},{"location":"#configuration-update-flow","text":"","title":"Configuration update flow"},{"location":"#how-this-looks-from-the-services-eyes","text":"This is roughly how configuration is consumed by a service. This paradigm encourages you to write software that can reconfigure itself in runtime rather than require a restart: Python #!/usr/bin/env python import grpc from v1.protoconf_service_pb2_grpc import ProtoconfServiceStub from v1.protoconf_service_pb2 import ConfigSubscriptionRequest from myproject.myconfig_pb2 import MyConfig channel = grpc . insecure_channel ( \"localhost:4300\" ) stub = ProtoconfServiceStub ( channel ) config = MyConfig () for update in stub . SubscribeForConfig ( ConfigSubscriptionRequest ( path = \"myproject/myconfig\" )): update . value . Unpack ( config ) print ( config ) As Protoconf uses Protobuf and gRPC, it supports delivering configuration to all major languages . See also: Protobuf overview .","title":"How this looks from the service's eyes"},{"location":"getting-started/","text":"Getting Started Prerequisite for this guide: Knowledge in Python Familiarity with Protobuf and gRPC Define your first config The first step will be to define the config struct in protobuf . The protobuf file will be used to generate a marshaler of in the language of choice which can be used alongside the gRPC client to pull configs from the protoconf agent gRPC endpoint. // file: ./src/myproject/myconfig.proto syntax = \"proto3\" ; message MyConfig { uint32 connection_timeout = 1 ; uint32 max_retries = 2 ; NestedStruct another_struct = 3 ; } message NestedStruct { string hello_world = 1 ; } Code your config Create a .pconf file to populate the config struct with the required values. \"\"\" file: ./src/myproject/myconfig.pconf \"\"\" load ( \"myconfig.proto\" , \"MyConfig\" , \"NestedStruct\" ) def main (): return MyConfig ( connection_timeout = 5 , max_retries = 5 , another_struct = NestedStruct ( hello_world = \"Hello World!\" ) ) Compile and check results Our working directory is now ready to be compiled. Run protoconf compile . . The compiler will create a new file under materialized_configs/myproject/myconfig.materialized_JSON which can be used to validate the result of the config. // f ile : ma ter ialized_co nf igs/myprojec t /myco nf ig.ma ter ialized_JSON { \"protoFile\" : \"myproject/myconfig.proto\" , \"value\" : { \"@type\" : \"type.googleapis.com/MyConfig\" , \"connectionTimeout\" : 5 , \"maxRetries\" : 5 , \"anotherStruct\" : { \"helloWorld\" : \"Hello World!\" } } } Add validators You might want to make sure no one can accidentally reduce the connection_timeout config below 3 . If he wish to do so, he can add a validator to the MyConfig struct: \"\"\" file: ./src/myproject/myconfig.proto-validator \"\"\" load ( \"myconfig.proto\" , \"MyConfig\" ) def validate_connection_timeout ( config ): if config . connection_timeout <= 3 : fail ( \"connection_timeout must be 3 or higher, got: %d \" % config . connection_timeout ) add_validator ( MyConfig , validate_connection_timeout ) Consume your config locally To test his configs locally, you can run protoconf agent -dev . The agent is now running and listening on 0.0.0.0:4300 and ready to accept gRPC calls. Install the grpc and protobuf tools to generate the stub code to communicate with the protoconf gRPC agent. $ pip install grpcio-tools $ python -m grpc_tools.protoc -Isrc --python_out = . --grpc_python_out = . ./src/myproject/myconfig.proto $ git clone https://github.com/protoconf/protoconf.git $ python -m grpc_tools.protoc -Iprotoconf/agent/api/proto/ --python_out = . --grpc_python_out = . protoconf/agent/api/proto/v1/protoconf_service.proto Write a simple python code that will use the generated code to communicate with the agent. #!/usr/bin/env python import grpc from v1.protoconf_service_pb2_grpc import ProtoconfServiceStub from v1.protoconf_service_pb2 import ConfigSubscriptionRequest from myproject.myconfig_pb2 import MyConfig channel = grpc . insecure_channel ( \"localhost:4300\" ) stub = ProtoconfServiceStub ( channel ) config = MyConfig () for update in stub . SubscribeForConfig ( ConfigSubscriptionRequest ( path = \"myproject/myconfig\" )): update . value . Unpack ( config ) print ( config ) Run the python code and make a change to the ./src/myproject/myconfig.pconf . After running protoconf compile . again, you will see the config changes in your running software. Prepare for Production Use a supported KV store to release the config to production. The supported storages are: Consul , Etcd or Zookeeper . $ consul agent -dev & $ protoconf insert -store consul -store-address localhost:8500 . myproject/myconfig Run the agent in production mode $ protoconf agent -store consul -store-address localhost:8500 Run your code the same way as step 5. Then make a change, compile and run the protoconf insert command from step 6 again.","title":"Getting Started"},{"location":"getting-started/#getting-started","text":"Prerequisite for this guide: Knowledge in Python Familiarity with Protobuf and gRPC","title":"Getting Started"},{"location":"getting-started/#define-your-first-config","text":"The first step will be to define the config struct in protobuf . The protobuf file will be used to generate a marshaler of in the language of choice which can be used alongside the gRPC client to pull configs from the protoconf agent gRPC endpoint. // file: ./src/myproject/myconfig.proto syntax = \"proto3\" ; message MyConfig { uint32 connection_timeout = 1 ; uint32 max_retries = 2 ; NestedStruct another_struct = 3 ; } message NestedStruct { string hello_world = 1 ; }","title":"Define your first config"},{"location":"getting-started/#code-your-config","text":"Create a .pconf file to populate the config struct with the required values. \"\"\" file: ./src/myproject/myconfig.pconf \"\"\" load ( \"myconfig.proto\" , \"MyConfig\" , \"NestedStruct\" ) def main (): return MyConfig ( connection_timeout = 5 , max_retries = 5 , another_struct = NestedStruct ( hello_world = \"Hello World!\" ) )","title":"Code your config"},{"location":"getting-started/#compile-and-check-results","text":"Our working directory is now ready to be compiled. Run protoconf compile . . The compiler will create a new file under materialized_configs/myproject/myconfig.materialized_JSON which can be used to validate the result of the config. // f ile : ma ter ialized_co nf igs/myprojec t /myco nf ig.ma ter ialized_JSON { \"protoFile\" : \"myproject/myconfig.proto\" , \"value\" : { \"@type\" : \"type.googleapis.com/MyConfig\" , \"connectionTimeout\" : 5 , \"maxRetries\" : 5 , \"anotherStruct\" : { \"helloWorld\" : \"Hello World!\" } } }","title":"Compile and check results"},{"location":"getting-started/#add-validators","text":"You might want to make sure no one can accidentally reduce the connection_timeout config below 3 . If he wish to do so, he can add a validator to the MyConfig struct: \"\"\" file: ./src/myproject/myconfig.proto-validator \"\"\" load ( \"myconfig.proto\" , \"MyConfig\" ) def validate_connection_timeout ( config ): if config . connection_timeout <= 3 : fail ( \"connection_timeout must be 3 or higher, got: %d \" % config . connection_timeout ) add_validator ( MyConfig , validate_connection_timeout )","title":"Add validators"},{"location":"getting-started/#consume-your-config-locally","text":"To test his configs locally, you can run protoconf agent -dev . The agent is now running and listening on 0.0.0.0:4300 and ready to accept gRPC calls. Install the grpc and protobuf tools to generate the stub code to communicate with the protoconf gRPC agent. $ pip install grpcio-tools $ python -m grpc_tools.protoc -Isrc --python_out = . --grpc_python_out = . ./src/myproject/myconfig.proto $ git clone https://github.com/protoconf/protoconf.git $ python -m grpc_tools.protoc -Iprotoconf/agent/api/proto/ --python_out = . --grpc_python_out = . protoconf/agent/api/proto/v1/protoconf_service.proto Write a simple python code that will use the generated code to communicate with the agent. #!/usr/bin/env python import grpc from v1.protoconf_service_pb2_grpc import ProtoconfServiceStub from v1.protoconf_service_pb2 import ConfigSubscriptionRequest from myproject.myconfig_pb2 import MyConfig channel = grpc . insecure_channel ( \"localhost:4300\" ) stub = ProtoconfServiceStub ( channel ) config = MyConfig () for update in stub . SubscribeForConfig ( ConfigSubscriptionRequest ( path = \"myproject/myconfig\" )): update . value . Unpack ( config ) print ( config ) Run the python code and make a change to the ./src/myproject/myconfig.pconf . After running protoconf compile . again, you will see the config changes in your running software.","title":"Consume your config locally"},{"location":"getting-started/#prepare-for-production","text":"Use a supported KV store to release the config to production. The supported storages are: Consul , Etcd or Zookeeper . $ consul agent -dev & $ protoconf insert -store consul -store-address localhost:8500 . myproject/myconfig","title":"Prepare for Production"},{"location":"getting-started/#run-the-agent-in-production-mode","text":"$ protoconf agent -store consul -store-address localhost:8500 Run your code the same way as step 5. Then make a change, compile and run the protoconf insert command from step 6 again.","title":"Run the agent in production mode"},{"location":"installation/","text":"Installation On Linux/MacOS export PROTOCONF_VERSION=\"0.1.4\" export PROTOCONF_OS=$(uname | tr '[A-Z]' '[a-z]') # change to \"arm64\" if needed export PROTOCONF_ARCH=\"amd64\" curl -LO https://github.com/protoconf/protoconf/releases/download/${PROTOCONF_VERSION}/protoconf-${PROTOCONF_OS}-${PROTOCONF_ARCH}-${PROTOCONF_VERSION}.tar.gz sudo tar xvf protoconf-${PROTOCONF_OS}-${PROTOCONF_ARCH}-${PROTOCONF_VERSION}.tar.gz -C /usr/local/bin On Windows Download from the github releases page . Validate the installation $ protoconf 2020/04/26 10:16:59 proto: duplicate proto type registered: v1.ConfigSubscriptionRequest 2020/04/26 10:16:59 proto: duplicate proto type registered: v1.ConfigUpdate Usage: protoconf [--version] [--help] <command> [<args>] Available commands are: agent Runs a Protoconf agent compile Compile configs exec Watches keys and execute on changes import insert Insert a materialized config to the key-value store mutate Write to mutation server serve Runs a server","title":"Installation"},{"location":"installation/#installation","text":"","title":"Installation"},{"location":"installation/#on-linuxmacos","text":"export PROTOCONF_VERSION=\"0.1.4\" export PROTOCONF_OS=$(uname | tr '[A-Z]' '[a-z]') # change to \"arm64\" if needed export PROTOCONF_ARCH=\"amd64\" curl -LO https://github.com/protoconf/protoconf/releases/download/${PROTOCONF_VERSION}/protoconf-${PROTOCONF_OS}-${PROTOCONF_ARCH}-${PROTOCONF_VERSION}.tar.gz sudo tar xvf protoconf-${PROTOCONF_OS}-${PROTOCONF_ARCH}-${PROTOCONF_VERSION}.tar.gz -C /usr/local/bin","title":"On Linux/MacOS"},{"location":"installation/#on-windows","text":"Download from the github releases page .","title":"On Windows"},{"location":"installation/#validate-the-installation","text":"$ protoconf 2020/04/26 10:16:59 proto: duplicate proto type registered: v1.ConfigSubscriptionRequest 2020/04/26 10:16:59 proto: duplicate proto type registered: v1.ConfigUpdate Usage: protoconf [--version] [--help] <command> [<args>] Available commands are: agent Runs a Protoconf agent compile Compile configs exec Watches keys and execute on changes import insert Insert a materialized config to the key-value store mutate Write to mutation server serve Runs a server","title":"Validate the installation"},{"location":"multiple-outputs/","text":"Multiple outputs Sometimes, we want to generate multiple configs from a single file. for this, we can use the .mpconf . protoconf expects .mpconf 's main() function to return a dict with a string as the key and a proto.Message as the value. Example: // file: src/myservice/myconfig.proto syntax = \"proto3\" ; message MyConfig { string name = 1 ; uint32 timeout = 2 ; } \"\"\" file: ./src/myservice/outputs.mpconf \"\"\" load ( \"myconfig.proto\" , \"MyConfig\" ) out = {} for i in range ( 5 ): out [ \"config %d \" % i ] = MyConfig ( name = \"config %d \" % i , timeout = i * 3 ) def main (): return out Now, when you compile your configs, you will see multiple outputs $ protoconf compile . $ find materialized_config/myservice materialized_config/myservice materialized_config/myservice/outputs materialized_config/myservice/outputs/config4.materialized_JSON materialized_config/myservice/outputs/config1.materialized_JSON materialized_config/myservice/outputs/config0.materialized_JSON materialized_config/myservice/outputs/config3.materialized_JSON materialized_config/myservice/outputs/config2.materialized_JSON","title":"Multiple Outputs"},{"location":"multiple-outputs/#multiple-outputs","text":"Sometimes, we want to generate multiple configs from a single file. for this, we can use the .mpconf . protoconf expects .mpconf 's main() function to return a dict with a string as the key and a proto.Message as the value. Example: // file: src/myservice/myconfig.proto syntax = \"proto3\" ; message MyConfig { string name = 1 ; uint32 timeout = 2 ; } \"\"\" file: ./src/myservice/outputs.mpconf \"\"\" load ( \"myconfig.proto\" , \"MyConfig\" ) out = {} for i in range ( 5 ): out [ \"config %d \" % i ] = MyConfig ( name = \"config %d \" % i , timeout = i * 3 ) def main (): return out Now, when you compile your configs, you will see multiple outputs $ protoconf compile . $ find materialized_config/myservice materialized_config/myservice materialized_config/myservice/outputs materialized_config/myservice/outputs/config4.materialized_JSON materialized_config/myservice/outputs/config1.materialized_JSON materialized_config/myservice/outputs/config0.materialized_JSON materialized_config/myservice/outputs/config3.materialized_JSON materialized_config/myservice/outputs/config2.materialized_JSON","title":"Multiple outputs"},{"location":"mutation-rpc/","text":"Mutation RPC usage One of the core principals of protoconf is the ability to mutate configs via an API (or RPC). This allows humans and machines work together on the same configuration codebase. protoconf allow humans to code the logic, while machines can only change values via the RPC. Create a dummy config // file: ./src/myservice/myconfig.proto syntax = \"proto3\" ; message MyConfig { string name = 1 ; uint32 timeout = 2 ; } \"\"\" file: ./src/myservice/default.pconf \"\"\" load ( \"myconfig.proto\" , \"MyConfig\" ) config = MyConfig ( name = \"config\" ) def main (): return config $ protoconf compile . Create a post-mutation script $ echo '#!/bin/bash\\nprotoconf compile .' > post.sh $ chmod +x post.sh Run the mutation server in the background $ protoconf serve -post ./post.sh . & Now we will use the protoconf mutate command to hit the mutation RPC $ protoconf mutate -path myservice/mutation -proto myservice/myconfig.proto -msg MyConfig -field timeout = 3 You will now notice a new file created under mutable_config $ find mutable_config mutable_config mutable_config/myservice mutable_config/myservice/mutation.materialized_JSON load the mutated values \"\"\" file: ./src/myservice/default.pconf \"\"\" load ( \"myconfig.proto\" , \"MyConfig\" ) load ( \"mutable:myservice/mutation\" , \"value\" ) config = MyConfig ( name = \"config\" , timeout = value . timeout ) def main (): return config Run the protoconf mutate command again with different value and watch how your configs changes. Next steps Running in production The protoconf serve command accepts -pre and -post scripts which should be used for preparing the ground for writing the mutation ( -pre ) and followup actions to run after writing the mutation ( -post ). Both scripts will run by protoconf serve on every mutation. The scripts will be receiveing a metadata string as its first argument ( $1 in bash ) and can be used to pass metadata from the initiator of the rpc to the script, This can be used to pass a token for github to validate the initiator or to pass more context to be added to the commit message. These scripts should handle the git lifecycle of the mutation (setting the workspace to latest ref, and push the result after the writing done.) compiling the configs should be part of the -post script. When running in HA, you can use these scripts to acquire a lock from consul / etcd . Using gRPC The mutation proto is available here .","title":"Mutation RPC"},{"location":"mutation-rpc/#mutation-rpc-usage","text":"One of the core principals of protoconf is the ability to mutate configs via an API (or RPC). This allows humans and machines work together on the same configuration codebase. protoconf allow humans to code the logic, while machines can only change values via the RPC.","title":"Mutation RPC usage"},{"location":"mutation-rpc/#create-a-dummy-config","text":"// file: ./src/myservice/myconfig.proto syntax = \"proto3\" ; message MyConfig { string name = 1 ; uint32 timeout = 2 ; } \"\"\" file: ./src/myservice/default.pconf \"\"\" load ( \"myconfig.proto\" , \"MyConfig\" ) config = MyConfig ( name = \"config\" ) def main (): return config $ protoconf compile .","title":"Create a dummy config"},{"location":"mutation-rpc/#create-a-post-mutation-script","text":"$ echo '#!/bin/bash\\nprotoconf compile .' > post.sh $ chmod +x post.sh","title":"Create a post-mutation script"},{"location":"mutation-rpc/#run-the-mutation-server-in-the-background","text":"$ protoconf serve -post ./post.sh . & Now we will use the protoconf mutate command to hit the mutation RPC $ protoconf mutate -path myservice/mutation -proto myservice/myconfig.proto -msg MyConfig -field timeout = 3 You will now notice a new file created under mutable_config $ find mutable_config mutable_config mutable_config/myservice mutable_config/myservice/mutation.materialized_JSON","title":"Run the mutation server in the background"},{"location":"mutation-rpc/#load-the-mutated-values","text":"\"\"\" file: ./src/myservice/default.pconf \"\"\" load ( \"myconfig.proto\" , \"MyConfig\" ) load ( \"mutable:myservice/mutation\" , \"value\" ) config = MyConfig ( name = \"config\" , timeout = value . timeout ) def main (): return config Run the protoconf mutate command again with different value and watch how your configs changes.","title":"load the mutated values"},{"location":"mutation-rpc/#next-steps","text":"","title":"Next steps"},{"location":"mutation-rpc/#running-in-production","text":"The protoconf serve command accepts -pre and -post scripts which should be used for preparing the ground for writing the mutation ( -pre ) and followup actions to run after writing the mutation ( -post ). Both scripts will run by protoconf serve on every mutation. The scripts will be receiveing a metadata string as its first argument ( $1 in bash ) and can be used to pass metadata from the initiator of the rpc to the script, This can be used to pass a token for github to validate the initiator or to pass more context to be added to the commit message. These scripts should handle the git lifecycle of the mutation (setting the workspace to latest ref, and push the result after the writing done.) compiling the configs should be part of the -post script. When running in HA, you can use these scripts to acquire a lock from consul / etcd .","title":"Running in production"},{"location":"mutation-rpc/#using-grpc","text":"The mutation proto is available here .","title":"Using gRPC"},{"location":"protoconf-exec/","text":"Using protoconf exec It's very likely that your infra relies on many components which does not have native protoconf integration. You can still use protoconf to code their config and use some wrappers around the process you are running in order to write and reload the config. protoconf exec aims to be a general purpose way to do so (still WIP and many features are not implemented) Import the exec config to your workspace $ mkdir -p src/exec curl -Lo src/exec/exec_config.proto https://raw.githubusercontent.com/protoconf/protoconf/v0.1.3/exec/config/exec_config.proto Create a dummy proto and config // file: ./src/myservice/myconfig.proto syntax = \"proto3\" ; message MyConfig { string name = 1 ; uint32 timeout = 2 ; } \"\"\" file: ./src/myservice/myconfig.pconf \"\"\" load ( \"myconfig.proto\" , \"MyConfig\" ) config = MyConfig ( name = \"test\" , timeout = 3 ) def main (): return config Create an exec config \"\"\" Generates tf.json files under ./tfconfigs \"\"\" load ( \"//exec/exec_config.proto\" , \"Config\" , \"WatcherConfig\" , \"Action\" , \"ActionTypeWriteToFile\" , ) configs = [ \"mysservice/myconfig\" ] def main (): return Config ( items = [ WatcherConfig ( path = path , proto_file = \"myservice/myconfig.proto\" , actions = [ Action ( file = ActionTypeWriteToFile ( path = \"tfconfigs/ %s .json\" % path )) ], ) for path in configs ] )","title":"Protoconf Exec"},{"location":"protoconf-exec/#using-protoconf-exec","text":"It's very likely that your infra relies on many components which does not have native protoconf integration. You can still use protoconf to code their config and use some wrappers around the process you are running in order to write and reload the config. protoconf exec aims to be a general purpose way to do so (still WIP and many features are not implemented)","title":"Using protoconf exec"},{"location":"protoconf-exec/#import-the-exec-config-to-your-workspace","text":"$ mkdir -p src/exec curl -Lo src/exec/exec_config.proto https://raw.githubusercontent.com/protoconf/protoconf/v0.1.3/exec/config/exec_config.proto","title":"Import the exec config to your workspace"},{"location":"protoconf-exec/#create-a-dummy-proto-and-config","text":"// file: ./src/myservice/myconfig.proto syntax = \"proto3\" ; message MyConfig { string name = 1 ; uint32 timeout = 2 ; } \"\"\" file: ./src/myservice/myconfig.pconf \"\"\" load ( \"myconfig.proto\" , \"MyConfig\" ) config = MyConfig ( name = \"test\" , timeout = 3 ) def main (): return config","title":"Create a dummy proto and config"},{"location":"protoconf-exec/#create-an-exec-config","text":"\"\"\" Generates tf.json files under ./tfconfigs \"\"\" load ( \"//exec/exec_config.proto\" , \"Config\" , \"WatcherConfig\" , \"Action\" , \"ActionTypeWriteToFile\" , ) configs = [ \"mysservice/myconfig\" ] def main (): return Config ( items = [ WatcherConfig ( path = path , proto_file = \"myservice/myconfig.proto\" , actions = [ Action ( file = ActionTypeWriteToFile ( path = \"tfconfigs/ %s .json\" % path )) ], ) for path in configs ] )","title":"Create an exec config"},{"location":"structuring-your-code/","text":"Structuring your code Sometimes, when we use protoconf we will want to write helpers functions and global constants that we might want to include in multiple configs. We can define those in .pinc files. .pinc files are starlark code which doesn't produce configs (doesn't evaluate the main() function). Example: \"\"\" file: ./src/helpers.pinc \"\"\" PROTOCONF_VERSION = \"0.1.3\" def format_name ( person ): # assumes `person` is a proto message that have `first_name` and `last_name` return \" %s %s \" % ( person . first_name , person . last_name ) We can now load the variables and functions in this file to another starlark file ( pinc , .pconf or .mpconf ) load ( \"//helpers.pinc\" , \"PROTOCONF_VERSION\" , \"format_name\" )","title":"Structure Your Code"},{"location":"structuring-your-code/#structuring-your-code","text":"Sometimes, when we use protoconf we will want to write helpers functions and global constants that we might want to include in multiple configs. We can define those in .pinc files. .pinc files are starlark code which doesn't produce configs (doesn't evaluate the main() function). Example: \"\"\" file: ./src/helpers.pinc \"\"\" PROTOCONF_VERSION = \"0.1.3\" def format_name ( person ): # assumes `person` is a proto message that have `first_name` and `last_name` return \" %s %s \" % ( person . first_name , person . last_name ) We can now load the variables and functions in this file to another starlark file ( pinc , .pconf or .mpconf ) load ( \"//helpers.pinc\" , \"PROTOCONF_VERSION\" , \"format_name\" )","title":"Structuring your code"},{"location":"integrations/terraform/","text":"Protoconf integration with Terraform Prerequists The terraform binray in your $PATH The protoconf binary in your $PATH Prepare Create a providers.tf file containing the providers declarations you need. provider \"random\" {} provider \"tls\" {} Initialize Terraform $ terraform init Copy all providers binaries to same directory $ find .terraform -type f -exec cp {} .terraform \\; Generate the terraform protos $ protoconf import terraform -import_path .terraform Validate the outpus $ ll src/terraform total 48K drwxr-xr-x 10 smintz 320 May 23 10 :46 . drwxr-xr-x 3 smintz 96 May 23 10 :39 .. -rw-r--r-- 1 smintz 461 May 23 10 :46 meta.proto -rw-r--r-- 1 smintz 51 May 23 10 :46 random-data.proto -rw-r--r-- 1 smintz 371 May 23 10 :46 random-provider.proto -rw-r--r-- 1 smintz 8 .1K May 23 10 :46 random-resources.proto -rw-r--r-- 1 smintz 4 .5K May 23 10 :46 terraform.proto -rw-r--r-- 1 smintz 1 .9K May 23 10 :46 tls-data.proto -rw-r--r-- 1 smintz 362 May 23 10 :46 tls-provider.proto -rw-r--r-- 1 smintz 6 .1K May 23 10 :46 tls-resources.proto The src/terraform/terraform.proto should looks like this: syntax = \"proto3\" ; package terraform ; import \"terraform/random-provider.proto\" ; import \"terraform/random-resources.proto\" ; import \"terraform/tls-data.proto\" ; import \"terraform/tls-provider.proto\" ; import \"terraform/tls-resources.proto\" ; message Terraform { Resources resource = 1 ; Datasources data = 2 ; Providers provider = 3 ; map < string , Variable > variable = 4 ; map < string , Output > output = 5 ; map < string , string > locals = 6 ; Module module = 7 ; TerraformSettings terraform = 8 ; message Resources { map < string , random.resources.RandomId > random_id = 1 [ json_name = \"random_id\" ]; map < string , random.resources.RandomInteger > random_integer = 2 [ json_name = \"random_integer\" ]; map < string , random.resources.RandomPassword > random_password = 3 [ json_name = \"random_password\" ]; map < string , random.resources.RandomPet > random_pet = 4 [ json_name = \"random_pet\" ]; map < string , random.resources.RandomShuffle > random_shuffle = 5 [ json_name = \"random_shuffle\" ]; map < string , random.resources.RandomString > random_string = 6 [ json_name = \"random_string\" ]; map < string , random.resources.RandomUuid > random_uuid = 7 [ json_name = \"random_uuid\" ]; map < string , tls.resources.TlsCertRequest > tls_cert_request = 8 [ json_name = \"tls_cert_request\" ]; map < string , tls.resources.TlsLocallySignedCert > tls_locally_signed_cert = 9 [ json_name = \"tls_locally_signed_cert\" ]; map < string , tls.resources.TlsPrivateKey > tls_private_key = 10 [ json_name = \"tls_private_key\" ]; map < string , tls.resources.TlsSelfSignedCert > tls_self_signed_cert = 11 [ json_name = \"tls_self_signed_cert\" ]; } message Datasources { map < string , tls.data.TlsCertificate > tls_certificate = 1 [ json_name = \"tls_certificate\" ]; map < string , tls.data.TlsPublicKey > tls_public_key = 2 [ json_name = \"tls_public_key\" ]; } message Providers { repeated random.provider.Random random = 1 ; repeated tls.provider.Tls tls = 2 ; } message Variable { string type = 1 ; string description = 2 ; string default = 3 ; } message Output { string value = 1 ; } message Module { } message TerraformSettings { string required_version = 1 [ json_name = \"required_version\" ]; map < string , Provider > required_providers = 2 ; Backend backend = 3 ; message Provider { string source = 1 ; string version = 2 ; } message Backend { oneof config { BackendLocal local = 1 ; BackendS3 s3 = 2 ; } message BackendLocal { string path = 1 ; string workspace_dir = 2 [ json_name = \"workspace_dir\" ]; } message BackendS3 { string region = 1 ; string access_key = 2 [ json_name = \"access_key\" ]; string secret_key = 3 [ json_name = \"secret_key\" ]; string iam_endpoint = 4 [ json_name = \"iam_endpoint\" ]; string max_retries = 5 [ json_name = \"max_retries\" ]; string profile = 6 ; string shared_credentials_file = 7 [ json_name = \"shared_credentials_file\" ]; string skip_credentials_validation = 8 [ json_name = \"skip_credentials_validation\" ]; string skip_region_validation = 9 [ json_name = \"skip_region_validation\" ]; string skip_metadata_api_check = 10 [ json_name = \"skip_metadata_api_check\" ]; string sts_endpoint = 11 [ json_name = \"sts_endpoint\" ]; string token = 12 ; string assume_role_duration_seconds = 13 [ json_name = \"assume_role_duration_seconds\" ]; string assume_role_policy = 14 [ json_name = \"assume_role_policy\" ]; string assume_role_policy_arns = 15 [ json_name = \"assume_role_policy_arns\" ]; string assume_role_tags = 16 [ json_name = \"assume_role_tags\" ]; string assume_role_transitive_tag_keys = 17 [ json_name = \"assume_role_transitive_tag_keys\" ]; string external_id = 18 [ json_name = \"external_id\" ]; string role_arn = 19 [ json_name = \"role_arn\" ]; string session_name = 20 [ json_name = \"session_name\" ]; string bucket = 21 ; string key = 22 ; string acl = 23 ; string encrypt = 24 ; string endpoint = 25 ; string force_path_style = 26 [ json_name = \"force_path_style\" ]; string kms_key_id = 27 [ json_name = \"kms_key_id\" ]; string sse_customer_key = 28 [ json_name = \"sse_customer_key\" ]; string workspace_key_prefix = 29 [ json_name = \"workspace_key_prefix\" ]; string dynamodb_endpoint = 30 [ json_name = \"dynamodb_endpoint\" ]; string dynamodb_table = 31 [ json_name = \"dynamodb_table\" ]; } } } } Create a .tf.pconf file # vim: filetype=python # ./src/tfdemo/tfdemo.tf.pconf load ( \"//terraform/terraform.proto\" , \"Terraform\" ) load ( \"//terraform/random-provider.proto\" , \"Random\" ) load ( \"//terraform/random-resources.proto\" , \"RandomPet\" ) tf = Terraform ( provider = Terraform . Providers ( random = [ Random ()]), resource = Terraform . Resources (), output = {}, ) tf . resource . random_pet [ \"my_dog_name\" ] = RandomPet () tf . output [ \"my_dog_name\" ] = Terraform . Output ( value = \"$ {random_pet.my_dog_name.id} \" ) def main (): return tf compile the config $ protoconf compile . Check the output cat materialized_config/tfdemo/tfdemo.tf.materialized_JSON | jq { \"protoFile\" : \"terraform/terraform.proto\" , \"value\" : { \"@type\" : \"type.googleapis.com/terraform.Terraform\" , \"output\" : { \"my_dog_name\" : { \"value\" : \"${random_pet.my_dog_name.id}\" } }, \"provider\" : { \"random\" : [ {} ] }, \"resource\" : { \"random_pet\" : { \"my_dog_name\" : {} } } } } prepare to run terraform $ mkdir tf $ cat materialized_config/tfdemo/tfdemo.tf.materialized_JSON | jq '.value | del(.[\"@type\"])' > tf/test.tf.json $ cd tf $ terraform init $ terraform plan An execution plan has been generated and is shown below. Resource actions are indicated with the following symbols: + create Terraform will perform the following actions: # random_pet.my_dog_name will be created + resource \"random_pet\" \"my_dog_name\" { + id = ( known after apply ) + length = 2 + separator = \"-\" } Plan: 1 to add, 0 to change, 0 to destroy. Changes to Outputs: + my_dog_name = ( known after apply ) ------------------------------------------------------------------------ Note: You didn 't specify an \"-out\" parameter to save this plan, so Terraform can' t guarantee that exactly these actions will be performed if \"terraform apply\" is subsequently run. $ terraform apply -auto-approve random_pet.my_dog_name: Creating... random_pet.my_dog_name: Creation complete after 0s [ id = key-zebra ] Apply complete! Resources: 1 added, 0 changed, 0 destroyed. Outputs: my_dog_name = \"key-zebra\"","title":"Terraform"},{"location":"integrations/terraform/#protoconf-integration-with-terraform","text":"","title":"Protoconf integration with Terraform"},{"location":"integrations/terraform/#prerequists","text":"The terraform binray in your $PATH The protoconf binary in your $PATH","title":"Prerequists"},{"location":"integrations/terraform/#prepare","text":"Create a providers.tf file containing the providers declarations you need. provider \"random\" {} provider \"tls\" {}","title":"Prepare"},{"location":"integrations/terraform/#initialize-terraform","text":"$ terraform init","title":"Initialize Terraform"},{"location":"integrations/terraform/#copy-all-providers-binaries-to-same-directory","text":"$ find .terraform -type f -exec cp {} .terraform \\;","title":"Copy all providers binaries to same directory"},{"location":"integrations/terraform/#generate-the-terraform-protos","text":"$ protoconf import terraform -import_path .terraform Validate the outpus $ ll src/terraform total 48K drwxr-xr-x 10 smintz 320 May 23 10 :46 . drwxr-xr-x 3 smintz 96 May 23 10 :39 .. -rw-r--r-- 1 smintz 461 May 23 10 :46 meta.proto -rw-r--r-- 1 smintz 51 May 23 10 :46 random-data.proto -rw-r--r-- 1 smintz 371 May 23 10 :46 random-provider.proto -rw-r--r-- 1 smintz 8 .1K May 23 10 :46 random-resources.proto -rw-r--r-- 1 smintz 4 .5K May 23 10 :46 terraform.proto -rw-r--r-- 1 smintz 1 .9K May 23 10 :46 tls-data.proto -rw-r--r-- 1 smintz 362 May 23 10 :46 tls-provider.proto -rw-r--r-- 1 smintz 6 .1K May 23 10 :46 tls-resources.proto The src/terraform/terraform.proto should looks like this: syntax = \"proto3\" ; package terraform ; import \"terraform/random-provider.proto\" ; import \"terraform/random-resources.proto\" ; import \"terraform/tls-data.proto\" ; import \"terraform/tls-provider.proto\" ; import \"terraform/tls-resources.proto\" ; message Terraform { Resources resource = 1 ; Datasources data = 2 ; Providers provider = 3 ; map < string , Variable > variable = 4 ; map < string , Output > output = 5 ; map < string , string > locals = 6 ; Module module = 7 ; TerraformSettings terraform = 8 ; message Resources { map < string , random.resources.RandomId > random_id = 1 [ json_name = \"random_id\" ]; map < string , random.resources.RandomInteger > random_integer = 2 [ json_name = \"random_integer\" ]; map < string , random.resources.RandomPassword > random_password = 3 [ json_name = \"random_password\" ]; map < string , random.resources.RandomPet > random_pet = 4 [ json_name = \"random_pet\" ]; map < string , random.resources.RandomShuffle > random_shuffle = 5 [ json_name = \"random_shuffle\" ]; map < string , random.resources.RandomString > random_string = 6 [ json_name = \"random_string\" ]; map < string , random.resources.RandomUuid > random_uuid = 7 [ json_name = \"random_uuid\" ]; map < string , tls.resources.TlsCertRequest > tls_cert_request = 8 [ json_name = \"tls_cert_request\" ]; map < string , tls.resources.TlsLocallySignedCert > tls_locally_signed_cert = 9 [ json_name = \"tls_locally_signed_cert\" ]; map < string , tls.resources.TlsPrivateKey > tls_private_key = 10 [ json_name = \"tls_private_key\" ]; map < string , tls.resources.TlsSelfSignedCert > tls_self_signed_cert = 11 [ json_name = \"tls_self_signed_cert\" ]; } message Datasources { map < string , tls.data.TlsCertificate > tls_certificate = 1 [ json_name = \"tls_certificate\" ]; map < string , tls.data.TlsPublicKey > tls_public_key = 2 [ json_name = \"tls_public_key\" ]; } message Providers { repeated random.provider.Random random = 1 ; repeated tls.provider.Tls tls = 2 ; } message Variable { string type = 1 ; string description = 2 ; string default = 3 ; } message Output { string value = 1 ; } message Module { } message TerraformSettings { string required_version = 1 [ json_name = \"required_version\" ]; map < string , Provider > required_providers = 2 ; Backend backend = 3 ; message Provider { string source = 1 ; string version = 2 ; } message Backend { oneof config { BackendLocal local = 1 ; BackendS3 s3 = 2 ; } message BackendLocal { string path = 1 ; string workspace_dir = 2 [ json_name = \"workspace_dir\" ]; } message BackendS3 { string region = 1 ; string access_key = 2 [ json_name = \"access_key\" ]; string secret_key = 3 [ json_name = \"secret_key\" ]; string iam_endpoint = 4 [ json_name = \"iam_endpoint\" ]; string max_retries = 5 [ json_name = \"max_retries\" ]; string profile = 6 ; string shared_credentials_file = 7 [ json_name = \"shared_credentials_file\" ]; string skip_credentials_validation = 8 [ json_name = \"skip_credentials_validation\" ]; string skip_region_validation = 9 [ json_name = \"skip_region_validation\" ]; string skip_metadata_api_check = 10 [ json_name = \"skip_metadata_api_check\" ]; string sts_endpoint = 11 [ json_name = \"sts_endpoint\" ]; string token = 12 ; string assume_role_duration_seconds = 13 [ json_name = \"assume_role_duration_seconds\" ]; string assume_role_policy = 14 [ json_name = \"assume_role_policy\" ]; string assume_role_policy_arns = 15 [ json_name = \"assume_role_policy_arns\" ]; string assume_role_tags = 16 [ json_name = \"assume_role_tags\" ]; string assume_role_transitive_tag_keys = 17 [ json_name = \"assume_role_transitive_tag_keys\" ]; string external_id = 18 [ json_name = \"external_id\" ]; string role_arn = 19 [ json_name = \"role_arn\" ]; string session_name = 20 [ json_name = \"session_name\" ]; string bucket = 21 ; string key = 22 ; string acl = 23 ; string encrypt = 24 ; string endpoint = 25 ; string force_path_style = 26 [ json_name = \"force_path_style\" ]; string kms_key_id = 27 [ json_name = \"kms_key_id\" ]; string sse_customer_key = 28 [ json_name = \"sse_customer_key\" ]; string workspace_key_prefix = 29 [ json_name = \"workspace_key_prefix\" ]; string dynamodb_endpoint = 30 [ json_name = \"dynamodb_endpoint\" ]; string dynamodb_table = 31 [ json_name = \"dynamodb_table\" ]; } } } }","title":"Generate the terraform protos"},{"location":"integrations/terraform/#create-a-tfpconf-file","text":"# vim: filetype=python # ./src/tfdemo/tfdemo.tf.pconf load ( \"//terraform/terraform.proto\" , \"Terraform\" ) load ( \"//terraform/random-provider.proto\" , \"Random\" ) load ( \"//terraform/random-resources.proto\" , \"RandomPet\" ) tf = Terraform ( provider = Terraform . Providers ( random = [ Random ()]), resource = Terraform . Resources (), output = {}, ) tf . resource . random_pet [ \"my_dog_name\" ] = RandomPet () tf . output [ \"my_dog_name\" ] = Terraform . Output ( value = \"$ {random_pet.my_dog_name.id} \" ) def main (): return tf","title":"Create a .tf.pconf file"},{"location":"integrations/terraform/#compile-the-config","text":"$ protoconf compile . Check the output cat materialized_config/tfdemo/tfdemo.tf.materialized_JSON | jq { \"protoFile\" : \"terraform/terraform.proto\" , \"value\" : { \"@type\" : \"type.googleapis.com/terraform.Terraform\" , \"output\" : { \"my_dog_name\" : { \"value\" : \"${random_pet.my_dog_name.id}\" } }, \"provider\" : { \"random\" : [ {} ] }, \"resource\" : { \"random_pet\" : { \"my_dog_name\" : {} } } } }","title":"compile the config"},{"location":"integrations/terraform/#prepare-to-run-terraform","text":"$ mkdir tf $ cat materialized_config/tfdemo/tfdemo.tf.materialized_JSON | jq '.value | del(.[\"@type\"])' > tf/test.tf.json $ cd tf $ terraform init $ terraform plan An execution plan has been generated and is shown below. Resource actions are indicated with the following symbols: + create Terraform will perform the following actions: # random_pet.my_dog_name will be created + resource \"random_pet\" \"my_dog_name\" { + id = ( known after apply ) + length = 2 + separator = \"-\" } Plan: 1 to add, 0 to change, 0 to destroy. Changes to Outputs: + my_dog_name = ( known after apply ) ------------------------------------------------------------------------ Note: You didn 't specify an \"-out\" parameter to save this plan, so Terraform can' t guarantee that exactly these actions will be performed if \"terraform apply\" is subsequently run. $ terraform apply -auto-approve random_pet.my_dog_name: Creating... random_pet.my_dog_name: Creation complete after 0s [ id = key-zebra ] Apply complete! Resources: 1 added, 0 changed, 0 destroyed. Outputs: my_dog_name = \"key-zebra\"","title":"prepare to run terraform"}]}